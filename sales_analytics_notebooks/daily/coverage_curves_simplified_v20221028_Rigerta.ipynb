{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd0edb-b4c3-4d0e-9a9a-4914dea966cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from math import floor\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39aec9c8-7fe5-4552-b3bd-fe50e9b5af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gitlabdata.orchestration_utils import (\n",
    "    query_dataframe,\n",
    "    query_executor,\n",
    "    snowflake_engine_factory,\n",
    "    data_science_engine_factory,\n",
    "    snowflake_stage_load_copy_remove,\n",
    "    dataframe_uploader\n",
    ")\n",
    "from os import environ as env\n",
    "\n",
    "snowflake_engine = snowflake_engine_factory(env, \"SALES_ANALYTICS\")\n",
    "branch_name = \"\"\n",
    "raw_db_name = env[\"SNOWFLAKE_LOAD_DATABASE\"]\n",
    "\n",
    "snowflake_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f91ac-f205-43d4-a2e7-54fc34ea11c4",
   "metadata": {},
   "source": [
    "# Model Description:\n",
    "\n",
    "## Goal\n",
    "\n",
    "Retrieve the quarterly coverage raw data & calculate fitted coverage curves for each level of aggregation\n",
    " \n",
    "## Steps\n",
    "\n",
    "- Retrieve data metrics per day models\n",
    "- Calculate the quarters needed for each metric\n",
    "- Integrate the different cuts into a single tall table\n",
    "- Pre-aggregate the data by business cut & calculate coverage\n",
    "- Using pre-aggregated data, fit a curve for each pre-aggregated model\n",
    "- Push changes to gSheet\n",
    "\n",
    "## Challenges with automation\n",
    "\n",
    "- How can we authenticate on gSheets without using my own auth json. (Maybe a service account within the library)?\n",
    "- How can we push data into Snowflake, ideally the workspace_sales directly from the jupyter workbook?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa37ee-913a-4bc3-b638-12c1841ed286",
   "metadata": {},
   "source": [
    "## Update to FY / 4Q historical curves\n",
    "\n",
    "List of steps:\n",
    "- Create metric per day for FY\n",
    "- How to create a metric day perspective for 4Q cuts?\n",
    "  - Starting quarter and end quarter\n",
    "  - Total booked on that time period\n",
    "  - Day 365 is the last day of end quarter\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf61a5c-bfb1-4468-bc34-9b1af47895c4",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e5a9314-f84e-48df-87cd-52797f727f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######\n",
    "## Create a fitter curve of the last 4 quarters for comparison\n",
    "#######\n",
    "def objective(x, a, b, c, d, e):\n",
    "    return a * x + b * x**2 + c * x**3 + d * x**4 + e\n",
    "\n",
    "def run_query_in_snowflake(conn, sql):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    df = cur.fetch_pandas_all()\n",
    "    return df\n",
    "\n",
    "def executeScriptFromFile(filename, snowflake_engine):\n",
    "    # Open and read the file as a single buffer\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "    \n",
    "    print(filename)\n",
    "    print(len(sqlFile))\n",
    "    \n",
    "    results = -1\n",
    "    \n",
    "    try:\n",
    "        results = query_dataframe(snowflake_engine,sqlFile)\n",
    "    except:\n",
    "        print(\"Command did not run\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def days_between(d1, d2):\n",
    "    #d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n",
    "    #d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n",
    "    return (d2 - d1).days\n",
    "\n",
    "\n",
    "def calculate_quarters_after_creation(x):\n",
    "        \n",
    "    age = 0\n",
    "    \n",
    "    if (x['IS_OPEN'] == 1):\n",
    "        age = days_between(x['CREATED_FISCAL_QUARTER_DATE'], x['SNAPSHOT_FISCAL_QUARTER_DATE'])\n",
    "    elif (x['IS_OPEN']== 0 and x['SNAPSHOT_DATE'] <= x['CLOSE_DATE']):\n",
    "        age = days_between(x['CREATED_FISCAL_QUARTER_DATE'], x['SNAPSHOT_FISCAL_QUARTER_DATE'])\n",
    "    else:\n",
    "        age = days_between(x['CREATED_FISCAL_QUARTER_DATE'], x['CLOSE_FISCAL_QUARTER_DATE'])\n",
    "    \n",
    "    quarter_delta = floor(age/90)\n",
    "    \n",
    "    return quarter_delta\n",
    "\n",
    "def calculate_channel_track (x):\n",
    "    \n",
    "    channel_track = 'Direct'\n",
    "    \n",
    "    if (x['deal_path'] == 'Direct'):\n",
    "        channel_track = 'Direct'\n",
    "    elif (x['deal_path'] == 'Web Direct'):\n",
    "        channel_track = 'Web Direct'\n",
    "    elif (x['deal_path'] == 'Channel'\n",
    "        and x['sales_qualified_source'] != 'Channel Generated'): \n",
    "        channel_track = 'Partner Co-Sell'\n",
    "    elif (x['deal_path'] == 'Channel'):\n",
    "        channel_track = 'Partner Sourced'\n",
    "    \n",
    "    return channel_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "177769ee-7fbd-4d1a-a841-79b9e72cefb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculates pending coverage using a minimum of 50k pending\n",
    "def calculate_to_pending_coverage (x, metric, qtd, actual, minimum_delta=5000):\n",
    "    \n",
    "    result = None\n",
    "    \n",
    "    actual = float(x[actual])\n",
    "    qtd = float(x[qtd])\n",
    "    metric = float(x[metric])\n",
    "    \n",
    "    if (actual - qtd) > minimum_delta:\n",
    "        result = metric / (actual - qtd)\n",
    "        result = min(result,6) # limiting the maximum amount of coverage to account for noise in the models\n",
    "    \n",
    "    return result\n",
    "\n",
    "# calculates pending coverage using a minimum of 50k pending\n",
    "def calculate_bookings_linearity (x, qtd_bookings, actual_booked):\n",
    "    \n",
    "    result = None\n",
    "    \n",
    "    actual = float(x[actual_booked])\n",
    "    qtd = float(x[qtd_bookings])\n",
    "       \n",
    "    if actual > 0:\n",
    "        result = qtd / actual \n",
    "        \n",
    "    return result \n",
    "\n",
    "\n",
    "# fits a curve to the subset data using the defined objective function\n",
    "def fit_curve_to_agg (data_agg, x_label, y_label):\n",
    "    \n",
    "    # fit a curve\n",
    "    x, y = data_agg[x_label], data_agg[y_label]\n",
    "    # curve fit\n",
    "    popt, _ = curve_fit(objective, x, y, method='dogbox')\n",
    "\n",
    "    x_line = np.arange(min(x), max(x), 1)\n",
    "    # calculate the output for the range\n",
    "    # summarize the parameter values\n",
    "    a, b, c , d, e = popt\n",
    "    y_line = objective(x_line, a, b, c, d, e)\n",
    "  \n",
    "    curve_result = pd.DataFrame({x_label:x_line,y_label:y_line})\n",
    "    return curve_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341e97b-d374-4d3d-8c90-da0b71a592b0",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ddce9e33-bb56-4aac-8f41-685291774950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### considered keys\n",
    "agg_key_list = ['key_segment','key_overall','sales_team_rd_asm_level',\n",
    "        'key_sqs', 'key_ot','key_segment_sqs', 'key_segment_ot', 'key_segment_geo', 'key_segment_geo_sqs',\n",
    "        'key_segment_geo_ot', 'key_segment_geo_region',\n",
    "        'key_segment_geo_region_sqs', 'key_segment_geo_region_ot',\n",
    "        'key_segment_geo_region_area', 'key_segment_geo_region_area_sqs','key_ent_comm',\n",
    "        'key_segment_geo_region_area_ot','report_user_segment_geo_region_area','key_alliance_partner',\n",
    "        'key_alliance_partner_ot',\n",
    "        'key_channel_category',\n",
    "        'key_channel_category_segment',\n",
    "        'key_channel_category_segment_geo',\n",
    "        'key_channel_category_ot',\n",
    "        'key_channel_category_segment_ot',\n",
    "        'key_channel_category_segment_geo_ot',\n",
    "        'key_segment_comcat',\n",
    "        'key_segment_geo_comcat'] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269a271-f2e3-4922-94c8-d93e1e23de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect quarterly data\n",
    "sql_file = 'qtr_metrics_by_day_channel'\n",
    "\n",
    "print (\"Executing {} Snowflake SQL\".format(sql_file))\n",
    "qtr_metrics_by_day = executeScriptFromFile(sql_file+'.sql', snowflake_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157c4a2-f8d7-4bd4-8e42-9c1c9987d98c",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "\n",
    "We collect the data and identify the quarters that we need for each metric.\n",
    "\n",
    "The way the model is construct, it will have for any given quarter values for open pipeline closing in the same quarter and in future quarters.\n",
    "\n",
    "To calculate the Current Quarter + 1 & + 2 metrics, we use the specific future field and filter the close quarter to the right point in time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887eba8-4eb1-408b-828a-ae209a9df4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform potential strings to date (in case of data read from CSV)\n",
    "qtr_metrics_by_day['close_fiscal_quarter_date'] = pd.to_datetime(qtr_metrics_by_day['close_fiscal_quarter_date']).dt.date\n",
    "qtr_metrics_by_day['rq_plus_1_close_fiscal_quarter_date'] = pd.to_datetime(qtr_metrics_by_day['rq_plus_1_close_fiscal_quarter_date']).dt.date\n",
    "qtr_metrics_by_day['rq_plus_2_close_fiscal_quarter_date'] = pd.to_datetime(qtr_metrics_by_day['rq_plus_2_close_fiscal_quarter_date']).dt.date\n",
    "\n",
    "#################\n",
    "##### Data Injections\n",
    "\n",
    "# create an artificial global key to calculate a global curve for the whole company\n",
    "qtr_metrics_by_day['key_overall'] = 'global'\n",
    "\n",
    "# adjust the sales_team legacy key to be sure that it won't generate duplicate with the FY23 key logic\n",
    "qtr_metrics_by_day['sales_team_rd_asm_level'] = 'st_rd_' + qtr_metrics_by_day['sales_team_rd_asm_level']\n",
    "\n",
    "# add a commercial and enterprise consolidated key\n",
    "#add key enterprise commercial for x-ray reporting\n",
    "qtr_metrics_by_day['key_ent_comm'] = 'other'\n",
    "qtr_metrics_by_day.loc[qtr_metrics_by_day['key_segment']=='large','key_ent_comm'] = 'enterprise'\n",
    "qtr_metrics_by_day.loc[qtr_metrics_by_day['key_segment']=='pubsec','key_ent_comm'] = 'enterprise'\n",
    "qtr_metrics_by_day.loc[qtr_metrics_by_day['key_segment']=='mid-market','key_ent_comm'] = 'commercial'\n",
    "qtr_metrics_by_day.loc[qtr_metrics_by_day['key_segment']=='smb','key_ent_comm'] = 'commercial'\n",
    "\n",
    "#################\n",
    "\n",
    "# identify the quarters that will be considered when creating the curves\n",
    "# for current quarter last 4 quarters\n",
    "index_cond = (qtr_metrics_by_day['close_fiscal_quarter_date'] <= date.today() + relativedelta(months=-3)) & (qtr_metrics_by_day['close_fiscal_quarter_date'] >= date.today() + relativedelta(months=-15))\n",
    "cq_considered_quarters = qtr_metrics_by_day[index_cond].close_fiscal_quarter_name.sort_values().unique()\n",
    "\n",
    "# for current quarter + 1, between 5 and 1 quarter ago (as we need the total won amount of the quarter to calculate coverage)\n",
    "index_cond = (qtr_metrics_by_day['close_fiscal_quarter_date'] <= date.today() + relativedelta(months=-6)) & (qtr_metrics_by_day['close_fiscal_quarter_date'] >= date.today() + relativedelta(months=-18))\n",
    "cq_1plus_considered_quarters = qtr_metrics_by_day[index_cond].close_fiscal_quarter_name.sort_values().unique()\n",
    "\n",
    "# for current quarter + 2, between 6 and 2 quarter ago (as we need the total won amount of the quarter to calculate coverage)\n",
    "index_cond = (qtr_metrics_by_day['close_fiscal_quarter_date'] <= date.today() + relativedelta(months=-9)) & (qtr_metrics_by_day['close_fiscal_quarter_date'] >= date.today() + relativedelta(months=-21))\n",
    "cq_2plus_considered_quarters = qtr_metrics_by_day[index_cond].close_fiscal_quarter_name.sort_values().unique()\n",
    "\n",
    "print(cq_considered_quarters)\n",
    "print(cq_1plus_considered_quarters)\n",
    "print(cq_2plus_considered_quarters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4701a9f8-befd-4b22-b44b-48b97fd47b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate considered data points per metric type\n",
    "cq_metrics_data = qtr_metrics_by_day[qtr_metrics_by_day['close_fiscal_quarter_name'].isin(cq_considered_quarters)].copy()\n",
    "cq_plus1_metrics_data = qtr_metrics_by_day[qtr_metrics_by_day['close_fiscal_quarter_name'].isin(cq_1plus_considered_quarters)].copy()\n",
    "cq_plus2_metrics_data = qtr_metrics_by_day[qtr_metrics_by_day['close_fiscal_quarter_name'].isin(cq_2plus_considered_quarters)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b024bc-9274-481e-af28-fe5815c61b34",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## From a wide table to a tall table\n",
    "\n",
    "After selecting and appending the relevant datasets into a single data with the same column name structure we unpivot the table into a tall one with a line per metric per day per fiscal quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f213b3-9116-4e5c-85b5-3161782ae98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are three fields that are critical\n",
    "# - Open Pipe Net ARR (Stage 1, 3, 4) (These can be metrics in a tall table)\n",
    "# - Booked Amount (This could be a column)\n",
    "# - Total Booked Amount (This can also be a column)\n",
    "\n",
    "\n",
    "### current quarter\n",
    "variables = ['close_day_of_fiscal_quarter_normalised','close_fiscal_quarter_name','total_booked_net_arr','booked_net_arr'] + agg_key_list\n",
    "considered_metrics = ['open_1plus_net_arr','open_3plus_net_arr','open_4plus_net_arr'] \n",
    "cq_melt = cq_metrics_data.melt(id_vars = variables, value_vars = considered_metrics)\n",
    "\n",
    "### current quarter + 1 \n",
    "variables = ['close_day_of_fiscal_quarter_normalised','close_fiscal_quarter_name','rq_plus_1_total_booked_net_arr','booked_net_arr'] + agg_key_list \n",
    "considered_metrics = ['rq_plus_1_open_1plus_net_arr', 'rq_plus_1_open_3plus_net_arr',\n",
    "       'rq_plus_1_open_4plus_net_arr']\n",
    "cq_plus1_melt = cq_plus1_metrics_data.melt(id_vars = variables, value_vars = considered_metrics)\n",
    "# rename the total booked field to keep it consistent\n",
    "cq_plus1_melt = cq_plus1_melt.rename({'rq_plus_1_total_booked_net_arr':'total_booked_net_arr'}, axis=1)\n",
    "cq_plus1_melt['booked_net_arr'] = 0\n",
    "\n",
    "### current quarter + 2\n",
    "variables = ['close_day_of_fiscal_quarter_normalised','close_fiscal_quarter_name','rq_plus_2_total_booked_net_arr','booked_net_arr'] + agg_key_list\n",
    "considered_metrics = ['rq_plus_2_open_1plus_net_arr', 'rq_plus_2_open_3plus_net_arr',\n",
    "       'rq_plus_2_open_4plus_net_arr']\n",
    "cq_plus2_melt = cq_plus2_metrics_data.melt(id_vars = variables, value_vars = considered_metrics)\n",
    "# rename the total booked field to keep it consistent\n",
    "cq_plus2_melt = cq_plus2_melt.rename({'rq_plus_2_total_booked_net_arr':'total_booked_net_arr'}, axis=1)\n",
    "cq_plus2_melt['booked_net_arr'] = 0\n",
    "\n",
    "### consolidated dataset\n",
    "combined_df = pd.concat([cq_melt, cq_plus1_melt,cq_plus2_melt], ignore_index=True)\n",
    "combined_df = combined_df.rename({'variable':'metric_name'}, axis=1)\n",
    "combined_df = combined_df.rename({'value':'metric_value'}, axis=1)\n",
    "\n",
    "##########################################################\n",
    "# to be able to calculate coverage, we need to drop:\n",
    "# - Lines where total booked net arr = 0\n",
    "# - Lines where value is NaN or 0\n",
    "# - Lines where booked_net_arr is NaN are to be set to 0\n",
    "\n",
    "###\n",
    "# - Lines where total booked net arr = 0\n",
    "# - Lines where value is NaN or 0\n",
    "combined_df.dropna(subset=['total_booked_net_arr', 'metric_value'], inplace=True)\n",
    "\n",
    "# - Lines where booked_net_arr is NaN are to be set to 0\n",
    "# combined_df = combined_df.drop(combined_df[combined_df['total_booked_net_arr']==0].index)\n",
    "len(combined_df)\n",
    "\n",
    "# test result\n",
    "print(len(combined_df), ' # of rows - ', combined_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d60f31-6668-43d4-8f4b-c53fd62584ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.metric_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b3aa14-64da-4467-92a5-29cc4453df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = (combined_df['metric_name']=='rq_plus_2_open_1plus_net_arr') & (combined_df['close_fiscal_quarter_name'] == 'FY22-Q1') & (combined_df['close_day_of_fiscal_quarter_normalised'] == 49) & (combined_df['key_segment_geo'] == 'large_emea')\n",
    "combined_df[index].groupby(['key_segment_geo'])['total_booked_net_arr'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34899261-4d19-454b-8c62-6edeba7734b6",
   "metadata": {},
   "source": [
    "## Calculation of coverages per Business cut\n",
    "\n",
    "For each business cut we need to do an aggregation of the relevant fields and from there run the coverage calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b854577d-271f-47d4-a813-904130920662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation columns\n",
    "agg_columns = ['metric_name','close_day_of_fiscal_quarter_normalised', 'close_fiscal_quarter_name']\n",
    "# fields to be summarized\n",
    "agg_fields = ['metric_value','total_booked_net_arr', 'booked_net_arr']\n",
    "\n",
    "# initialize to none, add. intermediate dataframes\n",
    "# the goal is to calcualte the coverage for each aggregation key. First the data needs to be grouped\n",
    "# at that level and the relevant fields summed up. Then the coverage calculation function is used\n",
    "combined_agg_results = None\n",
    "for agg_key in agg_key_list:\n",
    "    \n",
    "    # the key for aggregation needs to be complimented by the day key and the quarter key\n",
    "    total_agg_keys = [agg_key] + agg_columns\n",
    "\n",
    "    #calculate the aggregation per key\n",
    "    temp = combined_df.groupby(total_agg_keys,dropna=False)[agg_fields].agg('sum').reset_index().copy()\n",
    "        \n",
    "    # calculate coverages per metric, all are calculated using the \"Pending\" approach\n",
    "    # this needs to be done for every iteration \n",
    "    temp['metric_coverage'] = temp.apply(calculate_to_pending_coverage, axis=1,metric='metric_value',qtd='booked_net_arr',actual='total_booked_net_arr')\n",
    "\n",
    "    # clean up / add information regarding the agg key\n",
    "    temp = temp.rename({agg_key:'agg_key_value'},axis=1)\n",
    "    temp['agg_key_name'] = agg_key\n",
    "\n",
    "    #consolidate results\n",
    "    if combined_agg_results is None:\n",
    "        combined_agg_results = temp.copy()\n",
    "    else:\n",
    "        combined_agg_results = combined_agg_results.append(temp) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4370da-ff85-40f0-a10e-d24ddcfe6ee9",
   "metadata": {},
   "source": [
    "# Calculation of Curve Fit\n",
    "\n",
    "The previous step created a combined result data where we have a pre-aggregated and precalculated coverage for each value of the distinct aggregations keys.\n",
    "\n",
    "To use this dataset we must filter by the value of the key we want to use e.g. Large and plot the metric_coverage field. \n",
    "\n",
    "The dataset has 4 data points per metric, as we consider 4 past quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a0c3d12-b8ac-42da-a91f-dee98415ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Curve Fitting per Cut\n",
    "# for this to run I need to filter a) they aggregation level, b) the specific value at that aggregation level.\n",
    "# the result will be a table like key_agg, day_metric, \n",
    "\n",
    "#initialization\n",
    "combined_fitted_results = None\n",
    "\n",
    "#iterate through all the aggregated keys\n",
    "for agg_key_name in combined_agg_results.agg_key_name.unique():\n",
    "\n",
    "    #filter temporary data\n",
    "    temp_key_data = combined_agg_results[combined_agg_results['agg_key_name']==agg_key_name].copy()\n",
    "    \n",
    "    #iterate through the unique values of the specific aggregation\n",
    "    for agg_key_value in temp_key_data.agg_key_value.unique():\n",
    "        \n",
    "        # filter specific value per agg temporary data\n",
    "        temp_key_value_data = temp_key_data[temp_key_data['agg_key_value']==agg_key_value].copy()\n",
    "                \n",
    "        # set up the range of the quarter curves we want to report on\n",
    "        metrics_temp = pd.DataFrame({\"close_day_of_fiscal_quarter_normalised\":range(0,91)})\n",
    "\n",
    "        for metric_name in temp_key_value_data.metric_name.unique():\n",
    "            \n",
    "            # drop nas\n",
    "            temp_metric_data = temp_key_value_data[temp_key_value_data['metric_name']==metric_name].copy()\n",
    "            temp_metric_data.dropna(subset=['metric_coverage'], inplace=True)\n",
    "            temp_cuve = None\n",
    "            # avoid fitting curves for cuts that do not have enough data\n",
    "            if len(temp_metric_data) >= 90:     \n",
    "                temp_curve = fit_curve_to_agg (temp_metric_data,'close_day_of_fiscal_quarter_normalised','metric_coverage') \n",
    "                temp_curve.rename({'metric_coverage':metric_name+'_coverage'},inplace=1,axis=1)   \n",
    "                metrics_temp =  metrics_temp.merge(temp_curve, how='left', on='close_day_of_fiscal_quarter_normalised')                 \n",
    "            \n",
    "        # add the metric detail to the dataset\n",
    "        metrics_temp['agg_key_name'] = agg_key_name\n",
    "        metrics_temp['agg_key_value'] = agg_key_value\n",
    "        \n",
    "        #consolidate results\n",
    "        if combined_fitted_results is None:\n",
    "            combined_fitted_results = metrics_temp.copy()\n",
    "        else:\n",
    "            combined_fitted_results = combined_fitted_results.append(metrics_temp) \n",
    "        \n",
    "        # remove all nans so we can plot the charts\n",
    "        combined_fitted_results.dropna(inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8cefad-1a94-4fdf-a7cc-cbfe958036fb",
   "metadata": {},
   "source": [
    "# Linearity metric calculation - Data Extract & Fit\n",
    "\n",
    "For linearity we follow a similar process, preaggregate the metrics and then fit a curve to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54751a73-0bef-4320-8cd9-05dac918f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ data set is the only relevant for linearity calculation\n",
    "\n",
    "# columns to use for the group by (apart from the aggregation key)\n",
    "agg_columns = ['close_day_of_fiscal_quarter_normalised', 'close_fiscal_quarter_name']\n",
    "\n",
    "# fields to be summarized\n",
    "agg_fields = ['total_booked_net_arr', 'booked_net_arr']\n",
    "\n",
    "# initialize to none, add. intermediate dataframes\n",
    "# the goal is to calcualte the linearity for each aggregation key. First the data needs to be grouped\n",
    "# at that level and the relevant fields summed up. Then the coverage calculation function is used\n",
    "linearity_agg_results = None\n",
    "for agg_key in agg_key_list:\n",
    "    \n",
    "    # the key for aggregation needs to be complimented by the day key and the quarter key\n",
    "    total_agg_keys = [agg_key] + agg_columns\n",
    "\n",
    "    #calculate the aggregation per key\n",
    "    temp = cq_metrics_data.groupby(total_agg_keys,dropna=False)[agg_fields].agg('sum').reset_index().copy()\n",
    "        \n",
    "    # calculate coverages per metric, all are calculated using the \"Pending\" approach\n",
    "    # this needs to be done for every iteration \n",
    "    temp['bookings_linearity'] = temp.apply(calculate_bookings_linearity, axis=1,qtd_bookings='booked_net_arr',actual_booked='total_booked_net_arr')\n",
    "\n",
    "    # clean up / add information regarding the agg key\n",
    "    temp = temp.rename({agg_key:'agg_key_value'},axis=1)\n",
    "    temp['agg_key_name'] = agg_key\n",
    "\n",
    "    #consolidate results\n",
    "    if linearity_agg_results is None:\n",
    "        linearity_agg_results = temp.copy()\n",
    "    else:\n",
    "        linearity_agg_results = linearity_agg_results.append(temp) \n",
    "\n",
    "# set linearity 0 to NaN\n",
    "linearity_agg_results[linearity_agg_results['bookings_linearity']==0]=np.NaN\n",
    "\n",
    "# remove NaNs from dataset\n",
    "linearity_agg_results.dropna(subset=['bookings_linearity'],axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b6ca4c1-c26b-459f-b335-9d7184b66a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Curve Fitting per Cut\n",
    "# for this to run I need to filter a) they aggregation level, b) the specific value at that aggregation level.\n",
    "# the result will be a table like key_agg, day_metric, \n",
    "\n",
    "#initialization\n",
    "linearity_fitted_results = None\n",
    "\n",
    "#iterate through all the aggregated keys\n",
    "for agg_key_name in linearity_agg_results.agg_key_name.unique():\n",
    "\n",
    "    #filter temporary data\n",
    "    temp_key_data = linearity_agg_results[linearity_agg_results['agg_key_name']==agg_key_name].copy()\n",
    "    linearity_temp = None\n",
    "    \n",
    "    #iterate through the unique values of the specific aggregation\n",
    "    for agg_key_value in temp_key_data.agg_key_value.unique():\n",
    "        \n",
    "        # filter specific value per agg temporary data\n",
    "        temp_key_value_data = temp_key_data[temp_key_data['agg_key_value']==agg_key_value].copy()\n",
    "                \n",
    "        # set up the range of the quarter curves we want to report on\n",
    "        linearity_temp = pd.DataFrame({\"close_day_of_fiscal_quarter_normalised\":range(0,91)})\n",
    "                 \n",
    "        # drop nas\n",
    "        temp_linearity_data = temp_key_value_data.copy()\n",
    "        temp_cuve = None\n",
    "\n",
    "        # avoid fitting curves for cuts that do not have enough data\n",
    "        if len(temp_linearity_data) > 180:     \n",
    "            temp_curve = fit_curve_to_agg (temp_linearity_data,'close_day_of_fiscal_quarter_normalised','bookings_linearity') \n",
    "            linearity_temp =  linearity_temp.merge(temp_curve, how='left', on='close_day_of_fiscal_quarter_normalised')                 \n",
    "            \n",
    "        # add the metric detail to the dataset\n",
    "        linearity_temp['agg_key_name'] = agg_key_name\n",
    "        linearity_temp['agg_key_value'] = agg_key_value\n",
    "        \n",
    "        #consolidate results\n",
    "        if linearity_fitted_results is None:\n",
    "            linearity_fitted_results = linearity_temp.copy()\n",
    "        else:\n",
    "            linearity_fitted_results = linearity_fitted_results.append(linearity_temp) \n",
    "        \n",
    "# remove all nans so we can plot the charts\n",
    "linearity_fitted_results.dropna(subset=['bookings_linearity'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18e64e-51ed-4825-8e6c-b7a2da03f2db",
   "metadata": {},
   "source": [
    "# Upload tables to Snowflake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5410a501-bc34-4494-9a1d-abeda3bc1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve fitted data pre-formatting\n",
    "subset = ['open_1plus_net_arr_coverage', 'open_3plus_net_arr_coverage','open_4plus_net_arr_coverage','rq_plus_1_open_1plus_net_arr_coverage','rq_plus_1_open_3plus_net_arr_coverage','rq_plus_1_open_4plus_net_arr_coverage', 'rq_plus_2_open_1plus_net_arr_coverage','rq_plus_2_open_3plus_net_arr_coverage','rq_plus_2_open_4plus_net_arr_coverage']\n",
    "index = ~combined_fitted_results['agg_key_value'].str.contains('jihu')\n",
    "fitted_curves_to_upload = combined_fitted_results[index].dropna(subset=subset, how='all').copy()\n",
    "\n",
    "fitted_curves_to_upload['key_agg_day'] = fitted_curves_to_upload['agg_key_value'] + '_' + fitted_curves_to_upload['close_day_of_fiscal_quarter_normalised'].astype(str) \n",
    "fitted_curves_to_upload['last_updated_at'] = date.today()\n",
    "\n",
    "\n",
    "\n",
    "fields_order = ['key_agg_day','agg_key_name','agg_key_value','close_day_of_fiscal_quarter_normalised','open_1plus_net_arr_coverage', 'open_3plus_net_arr_coverage','open_4plus_net_arr_coverage','rq_plus_1_open_1plus_net_arr_coverage','rq_plus_1_open_3plus_net_arr_coverage','rq_plus_1_open_4plus_net_arr_coverage', 'rq_plus_2_open_1plus_net_arr_coverage','rq_plus_2_open_3plus_net_arr_coverage','rq_plus_2_open_4plus_net_arr_coverage','last_updated_at']\n",
    "\n",
    "# columns in the CSV MUST BE UPPERCASE, if not, the drive process might fail\n",
    "fitted_curves_to_upload = fitted_curves_to_upload[fields_order].copy()\n",
    "fitted_curves_to_upload.columns = fitted_curves_to_upload.columns.str.upper()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35c614dd-92bb-4cbc-8a38-7999cccadc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "curves_fitted_table = 'hist_quarterly_coverage_fitted_curves'\n",
    "\n",
    "# re-create the engine with a different profile that has write permissions on the sales analytics schema\n",
    "engine = snowflake_engine_factory(env, \"SALES_ANALYTICS\")\n",
    "\n",
    "dataframe_uploader(\n",
    "    dataframe = fitted_curves_to_upload.head(),\n",
    "    engine = engine,\n",
    "    table_name = curves_fitted_table,\n",
    "    schema = \"SALES_ANALYTICS\",\n",
    "    if_exists = \"replace\",\n",
    "    add_uploaded_at = False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea4f0e1d1faa5bb96f695d7e31ce7a9817f73b9836258768e692d46b900aef52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
