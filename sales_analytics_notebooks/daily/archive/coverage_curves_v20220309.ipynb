{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5554610-3b12-4172-a7b7-f933f10c34da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install  pygsheets\n",
    "!{sys.executable} -m pip install \"pyarrow<5.1.0,>=5.0.0;\"\n",
    "!{sys.executable} -m pip install  pydrive\n",
    "\n",
    "#get data\n",
    "import pygsheets\n",
    "import configparser\n",
    "import sys\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import numpy as np                   # v 1.19.2\n",
    "import matplotlib.pyplot as plt      # v 3.3.2\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from math import floor \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# link to snowflake library\n",
    "from gitlabdata.orchestration_utils import (\n",
    "    data_science_engine_factory,\n",
    "    query_dataframe,\n",
    ")\n",
    "\n",
    "engine = data_science_engine_factory()\n",
    "\n",
    "\n",
    "#target gsheet for uploads and downloads, just allowing one to keep it more secure\n",
    "# x-ray source target id\n",
    "x_ray_spreadsheet_id = '1Vwu8euxRgIF3QYWK8hAbp4Vy21AlFfpDwI4MaEEiIWk'\n",
    "\n",
    "# the following gsheets are used to push the data into the snowflake\n",
    "curves_sheetload_spreadsheet_id = '1dLevdYA8QMjpIV9irNGD8KTfKIgc3xJdrRDA2vH7M50'\n",
    "\n",
    "#key to be able to work with bigquery / gsheets\n",
    "service_file_path = '/Volumes/GoogleDrive-101341343143168722397/My Drive/docker_work/jupyter_analysis/20220210_FY22_channel_value_analysis/nfiguera-c3fe9e64-a6543dd51e79.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f91ac-f205-43d4-a2e7-54fc34ea11c4",
   "metadata": {},
   "source": [
    "# Model Description:\n",
    "\n",
    "## Goal\n",
    "\n",
    "Retrieve the quarterly coverage raw data & calculate fitted coverage curves for each level of aggregation\n",
    " \n",
    "## Steps\n",
    "\n",
    "- Retrieve data metrics per day models\n",
    "- Calculate the quarters needed for each metric\n",
    "- Integrate the different cuts into a single tall table\n",
    "- Pre-aggregate the data by business cut & calculate coverage\n",
    "- Using pre-aggregated data, fit a curve for each pre-aggregated model\n",
    "- Push changes to gSheet\n",
    "\n",
    "## Challenges with automation\n",
    "\n",
    "- How can we authenticate on gSheets without using my own auth json. (Maybe a service account within the library)?\n",
    "- How can we push data into Snowflake, ideally the workspace_sales directly from the jupyter workbook?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4277ca6-9492-41d0-89dc-7683108e3d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "os.getcwd()\n",
    "\n",
    "# NF: Just to deal with my working directory changing\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "os.chdir(\"/Volumes/GoogleDrive-101341343143168722397/My Drive/docker_work/jupyter_analysis/20220223_Targets_&_CoverageCurves\")\n",
    "\n",
    "#how to add access to gsheet\n",
    "#https://stackoverflow.com/questions/62917910/python-export-pandas-dataframe-to-google-sheets-solved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014121ff-57ae-45c7-807a-a74f74ae0505",
   "metadata": {},
   "source": [
    "\n",
    "import pydrive\n",
    "\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "gauth = pydrive.auth.GoogleAuth(service_file_path).LoadClientConfigFile(service_file_path)\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'https://drive.google.com/file/d/1RIuSxxpd3Q_2bfpxPng5fu3Oaa3wqC0j'}) # replace the id with id of the file you want to access\n",
    "downloaded.GetContentFile('file.csv')  \n",
    "GoogleAuth(service_file_path)\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'https://drive.google.com/file/d/1RIuSxxpd3Q_2bfpxPng5fu3Oaa3wqC0j'}) # replace the id with id of the file you want to access\n",
    "downloaded.GetContentFile('file.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf61a5c-bfb1-4468-bc34-9b1af47895c4",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a9314-f84e-48df-87cd-52797f727f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "## Create a fitter curve of the last 4 quarters for comparison\n",
    "#######\n",
    "def objective(x, a, b, c, d, e):\n",
    "    return a * x + b * x**2 + c * x**3 + d * x**4 + e\n",
    "\n",
    "\n",
    "def write_to_gsheet(service_file_path, spreadsheet_id, sheet_name, data_df):\n",
    "    \"\"\"\n",
    "    this function takes data_df and writes it under spreadsheet_id\n",
    "    and sheet_name using your credentials under service_file_path\n",
    "    \"\"\"\n",
    "    gc = pygsheets.authorize(service_file=service_file_path)\n",
    "    sh = gc.open_by_key(spreadsheet_id)\n",
    "    try:\n",
    "        sh.add_worksheet(sheet_name)\n",
    "    except:\n",
    "        pass\n",
    "    wks_write = sh.worksheet_by_title(sheet_name)\n",
    "    wks_write.clear('A1',None,'*')\n",
    "    wks_write.set_dataframe(data_df, (1,1), encoding='utf-8', fit=True)\n",
    "    wks_write.frozen_rows = 1\n",
    "\n",
    "def read_from_gsheet(service_file_path, spreadsheet_id, sheet_name):\n",
    "    \"\"\"\n",
    "    this function takes a sheet_name from a spreadsheet_id and returns a data frame \n",
    "    \"\"\"\n",
    "    gc = pygsheets.authorize(service_file=service_file_path)\n",
    "    sh = gc.open_by_key(spreadsheet_id)\n",
    "   \n",
    "    wks_read = sh.worksheet_by_title(sheet_name)\n",
    "    read = wks_read.get_as_df()\n",
    "    \n",
    "    return read\n",
    "\n",
    "\n",
    "def run_query_in_snowflake(conn, sql):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    df = cur.fetch_pandas_all()\n",
    "    return df\n",
    "\n",
    "def executeScriptFromFile(filename, engine):\n",
    "    # Open and read the file as a single buffer\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "    \n",
    "    results = -1\n",
    "    \n",
    "    try:\n",
    "        results = query_dataframe(engine,sqlFile)\n",
    "    except:\n",
    "        print(\"Command did not run\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def days_between(d1, d2):\n",
    "    #d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n",
    "    #d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n",
    "    return (d2 - d1).days\n",
    "\n",
    "\n",
    "def calculate_quarters_after_creation(x):\n",
    "        \n",
    "    age = 0\n",
    "    \n",
    "    if (x['IS_OPEN'] == 1):\n",
    "        age = days_between(x['CREATED_FISCAL_QUARTER_DATE'], x['SNAPSHOT_FISCAL_QUARTER_DATE'])\n",
    "    elif (x['IS_OPEN']== 0 and x['SNAPSHOT_DATE'] <= x['CLOSE_DATE']):\n",
    "        age = days_between(x['CREATED_FISCAL_QUARTER_DATE'], x['SNAPSHOT_FISCAL_QUARTER_DATE'])\n",
    "    else:\n",
    "        age = days_between(x['CREATED_FISCAL_QUARTER_DATE'], x['CLOSE_FISCAL_QUARTER_DATE'])\n",
    "    \n",
    "    quarter_delta = floor(age/90)\n",
    "    \n",
    "    return quarter_delta\n",
    "\n",
    "def calculate_channel_track (x):\n",
    "    \n",
    "    channel_track = 'Direct'\n",
    "    \n",
    "    if (x['deal_path'] == 'Direct'):\n",
    "        channel_track = 'Direct'\n",
    "    elif (x['deal_path'] == 'Web Direct'):\n",
    "        channel_track = 'Web Direct'\n",
    "    elif (x['deal_path'] == 'Channel'\n",
    "        and x['sales_qualified_source'] != 'Channel Generated'): \n",
    "        channel_track = 'Partner Co-Sell'\n",
    "    elif (x['deal_path'] == 'Channel'):\n",
    "        channel_track = 'Partner Sourced'\n",
    "    \n",
    "    return channel_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177769ee-7fbd-4d1a-a841-79b9e72cefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates pending coverage using a minimum of 50k pending\n",
    "def calculate_to_pending_coverage (x, metric, qtd, actual, minimum_delta=5000):\n",
    "    \n",
    "    result = None\n",
    "    \n",
    "    actual = float(x[actual])\n",
    "    qtd = float(x[qtd])\n",
    "    metric = float(x[metric])\n",
    "    \n",
    "    if (actual - qtd) > minimum_delta:\n",
    "        result = metric / (actual - qtd)\n",
    "        result = min(result,6) # limiting the maximum amount of coverage to account for noise in the models\n",
    "    \n",
    "    return result\n",
    "\n",
    "# calculates pending coverage using a minimum of 50k pending\n",
    "def calculate_bookings_linearity (x, qtd_bookings, actual_booked):\n",
    "    \n",
    "    result = None\n",
    "    \n",
    "    actual = float(x[actual_booked])\n",
    "    qtd = float(x[qtd_bookings])\n",
    "       \n",
    "    if actual > 0:\n",
    "        result = qtd / actual \n",
    "        \n",
    "    return result \n",
    "\n",
    "\n",
    "# fits a curve to the subset data using the defined objective function\n",
    "def fit_curve_to_agg (data_agg, x_label, y_label):\n",
    "    \n",
    "    # fit a curve\n",
    "    x, y = data_agg[x_label], data_agg[y_label]\n",
    "    # curve fit\n",
    "    popt, _ = curve_fit(objective, x, y, method='dogbox')\n",
    "\n",
    "    x_line = np.arange(min(x), max(x), 1)\n",
    "    # calculate the output for the range\n",
    "    # summarize the parameter values\n",
    "    a, b, c , d, e = popt\n",
    "    y_line = objective(x_line, a, b, c, d, e)\n",
    "  \n",
    "    curve_result = pd.DataFrame({x_label:x_line,y_label:y_line})\n",
    "    return curve_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341e97b-d374-4d3d-8c90-da0b71a592b0",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a6c5ba-be95-40eb-9bf2-cce0319952a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_day_db = executeScriptFromFile('metrics_by_day.sql', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6cf59d-cb72-4a6f-a73b-90006307d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_by_day_db.to_csv('metrics_by_day.csv',index=False)\n",
    "#metrics_by_day_db = pd.read_csv('metrics_by_day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ad23d-3f95-4446-ab0d-2958c1199ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted quarters\n",
    "metrics_by_day_db.close_fiscal_quarter_name.sort_values().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6646989a-9f45-4661-b166-1bc4e80cfd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the total amount of pipe gen in a given quarter\n",
    "metrics_by_day_db[(metrics_by_day_db['close_fiscal_quarter_name'] == 'FY23-Q1') & (metrics_by_day_db['close_day_of_fiscal_quarter_normalised'] == 23)].pipe_gen_net_arr.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157c4a2-f8d7-4bd4-8e42-9c1c9987d98c",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "\n",
    "We collect the data and identify the quarters that we need for each metric.\n",
    "\n",
    "The way the model is construct, it will have for any given quarter values for open pipeline closing in the same quarter and in future quarters.\n",
    "\n",
    "To calculate the Current Quarter + 1 & + 2 metrics, we use the specific future field and filter the close quarter to the right point in time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887eba8-4eb1-408b-828a-ae209a9df4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "##### Data Injections\n",
    "\n",
    "# create an artificial global key to calculate a global curve for the whole company\n",
    "metrics_by_day_db['key_overall'] = 'global'\n",
    "\n",
    "# adjust the sales_team legacy key to be sure that it won't generate duplicate with the FY23 key logic\n",
    "metrics_by_day_db['sales_team_rd_asm_level'] = 'st_rd_' + metrics_by_day_db['sales_team_rd_asm_level']\n",
    "\n",
    "# add a commercial and enterprise consolidated key\n",
    "#add key enterprise commercial for x-ray reporting\n",
    "metrics_by_day_db['key_ent_comm'] = 'other'\n",
    "metrics_by_day_db.loc[metrics_by_day_db['key_segment']=='large','key_ent_comm'] = 'enterprise'\n",
    "metrics_by_day_db.loc[metrics_by_day_db['key_segment']=='pubsec','key_ent_comm'] = 'enterprise'\n",
    "metrics_by_day_db.loc[metrics_by_day_db['key_segment']=='mid-market','key_ent_comm'] = 'commercial'\n",
    "metrics_by_day_db.loc[metrics_by_day_db['key_segment']=='smb','key_ent_comm'] = 'commercial'\n",
    "\n",
    "#################\n",
    "\n",
    "# identify the quarters that will be considered when creating the curves\n",
    "# for current quarter last 4 quarters\n",
    "index_cond = (metrics_by_day_db['close_fiscal_quarter_date'] <= date.today() + relativedelta(months=-3)) & (metrics_by_day_db['close_fiscal_quarter_date'] >= date.today() + relativedelta(months=-15))\n",
    "cq_considered_quarters = metrics_by_day_db[index_cond].close_fiscal_quarter_name.sort_values().unique()\n",
    "\n",
    "# for current quarter + 1, between 5 and 1 quarter ago (as we need the total won amount of the quarter to calculate coverage)\n",
    "index_cond = (metrics_by_day_db['close_fiscal_quarter_date'] <= date.today() + relativedelta(months=-6)) & (metrics_by_day_db['close_fiscal_quarter_date'] >= date.today() + relativedelta(months=-18))\n",
    "cq_1plus_considered_quarters = metrics_by_day_db[index_cond].close_fiscal_quarter_name.sort_values().unique()\n",
    "\n",
    "# for current quarter + 2, between 6 and 2 quarter ago (as we need the total won amount of the quarter to calculate coverage)\n",
    "index_cond = (metrics_by_day_db['close_fiscal_quarter_date'] <= date.today() + relativedelta(months=-9)) & (metrics_by_day_db['close_fiscal_quarter_date'] >= date.today() + relativedelta(months=-21))\n",
    "cq_2plus_considered_quarters = metrics_by_day_db[index_cond].close_fiscal_quarter_name.sort_values().unique()\n",
    "\n",
    "print(cq_considered_quarters)\n",
    "print(cq_1plus_considered_quarters)\n",
    "print(cq_2plus_considered_quarters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701a9f8-befd-4b22-b44b-48b97fd47b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate considered data points per metric type\n",
    "cq_metrics_data = metrics_by_day_db[metrics_by_day_db['close_fiscal_quarter_name'].isin(cq_considered_quarters)].copy()\n",
    "cq_plus1_metrics_data = metrics_by_day_db[metrics_by_day_db['close_fiscal_quarter_name'].isin(cq_1plus_considered_quarters)].copy()\n",
    "cq_plus2_metrics_data = metrics_by_day_db[metrics_by_day_db['close_fiscal_quarter_name'].isin(cq_2plus_considered_quarters)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b024bc-9274-481e-af28-fe5815c61b34",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## From a wide table to a tall table\n",
    "\n",
    "After selecting and appending the relevant datasets into a single data with the same column name structure we unpivot the table into a tall one with a line per metric per day per fiscal quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f213b3-9116-4e5c-85b5-3161782ae98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are three fields that are critical\n",
    "# - Open Pipe Net ARR (Stage 1, 3, 4) (These can be metrics in a tall table)\n",
    "# - Booked Amount (This could be a column)\n",
    "# - Total Booked Amount (This can also be a column)\n",
    "\n",
    "### considered keys\n",
    "agg_key_list = ['key_segment','key_overall','sales_team_rd_asm_level',\n",
    "       'key_sqs', 'key_ot','key_segment_sqs', 'key_segment_ot', 'key_segment_geo', 'key_segment_geo_sqs',\n",
    "       'key_segment_geo_ot', 'key_segment_geo_region',\n",
    "       'key_segment_geo_region_sqs', 'key_segment_geo_region_ot',\n",
    "       'key_segment_geo_region_area', 'key_segment_geo_region_area_sqs','key_ent_comm',\n",
    "       'key_segment_geo_region_area_ot','report_user_segment_geo_region_area'] #\n",
    "\n",
    "### current quarter\n",
    "variables = ['close_day_of_fiscal_quarter_normalised','close_fiscal_quarter_name','total_booked_net_arr','booked_net_arr'] + agg_key_list\n",
    "considered_metrics = ['open_1plus_net_arr','open_3plus_net_arr','open_4plus_net_arr'] \n",
    "cq_melt = cq_metrics_data.melt(id_vars = variables, value_vars = considered_metrics)\n",
    "\n",
    "### current quarter + 1 \n",
    "variables = ['close_day_of_fiscal_quarter_normalised','close_fiscal_quarter_name','rq_plus_1_total_booked_net_arr','booked_net_arr'] + agg_key_list \n",
    "considered_metrics = ['rq_plus_1_open_1plus_net_arr', 'rq_plus_1_open_3plus_net_arr',\n",
    "       'rq_plus_1_open_4plus_net_arr']\n",
    "cq_plus1_melt = cq_plus1_metrics_data.melt(id_vars = variables, value_vars = considered_metrics)\n",
    "# rename the total booked field to keep it consistent\n",
    "cq_plus1_melt = cq_plus1_melt.rename({'rq_plus_1_total_booked_net_arr':'total_booked_net_arr'}, axis=1)\n",
    "cq_plus1_melt['booked_net_arr'] = 0\n",
    "\n",
    "### current quarter + 2\n",
    "variables = ['close_day_of_fiscal_quarter_normalised','close_fiscal_quarter_name','rq_plus_2_total_booked_net_arr','booked_net_arr'] + agg_key_list\n",
    "considered_metrics = ['rq_plus_2_open_1plus_net_arr', 'rq_plus_2_open_3plus_net_arr',\n",
    "       'rq_plus_2_open_4plus_net_arr']\n",
    "cq_plus2_melt = cq_plus2_metrics_data.melt(id_vars = variables, value_vars = considered_metrics)\n",
    "# rename the total booked field to keep it consistent\n",
    "cq_plus2_melt = cq_plus2_melt.rename({'rq_plus_2_total_booked_net_arr':'total_booked_net_arr'}, axis=1)\n",
    "cq_plus2_melt['booked_net_arr'] = 0\n",
    "\n",
    "### consolidated dataset\n",
    "combined_df = pd.concat([cq_melt, cq_plus1_melt,cq_plus2_melt], ignore_index=True)\n",
    "combined_df = combined_df.rename({'variable':'metric_name'}, axis=1)\n",
    "combined_df = combined_df.rename({'value':'metric_value'}, axis=1)\n",
    "\n",
    "##########################################################\n",
    "# to be able to calculate coverage, we need to drop:\n",
    "# - Lines where total booked net arr = 0\n",
    "# - Lines where value is NaN or 0\n",
    "# - Lines where booked_net_arr is NaN are to be set to 0\n",
    "\n",
    "###\n",
    "# - Lines where total booked net arr = 0\n",
    "# - Lines where value is NaN or 0\n",
    "combined_df.dropna(subset=['total_booked_net_arr', 'metric_value'], inplace=True)\n",
    "\n",
    "# - Lines where booked_net_arr is NaN are to be set to 0\n",
    "# combined_df = combined_df.drop(combined_df[combined_df['total_booked_net_arr']==0].index)\n",
    "len(combined_df)\n",
    "\n",
    "# test result\n",
    "print(len(combined_df), ' # of rows - ', combined_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34899261-4d19-454b-8c62-6edeba7734b6",
   "metadata": {},
   "source": [
    "## Calculation of coverages per Business cut\n",
    "\n",
    "For each business cut we need to do an aggregation of the relevant fields and from there run the coverage calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854577d-271f-47d4-a813-904130920662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation columns\n",
    "agg_columns = ['metric_name','close_day_of_fiscal_quarter_normalised', 'close_fiscal_quarter_name']\n",
    "# fields to be summarized\n",
    "agg_fields = ['metric_value','total_booked_net_arr', 'booked_net_arr']\n",
    "\n",
    "# initialize to none, add. intermediate dataframes\n",
    "# the goal is to calcualte the coverage for each aggregation key. First the data needs to be grouped\n",
    "# at that level and the relevant fields summed up. Then the coverage calculation function is used\n",
    "combined_agg_results = None\n",
    "for agg_key in agg_key_list:\n",
    "    \n",
    "    # the key for aggregation needs to be complimented by the day key and the quarter key\n",
    "    total_agg_keys = [agg_key] + agg_columns\n",
    "\n",
    "    #calculate the aggregation per key\n",
    "    temp = combined_df.groupby(total_agg_keys,dropna=False)[agg_fields].agg('sum').reset_index().copy()\n",
    "        \n",
    "    # calculate coverages per metric, all are calculated using the \"Pending\" approach\n",
    "    # this needs to be done for every iteration \n",
    "    temp['metric_coverage'] = temp.apply(calculate_to_pending_coverage, axis=1,metric='metric_value',qtd='booked_net_arr',actual='total_booked_net_arr')\n",
    "\n",
    "    # clean up / add information regarding the agg key\n",
    "    temp = temp.rename({agg_key:'agg_key_value'},axis=1)\n",
    "    temp['agg_key_name'] = agg_key\n",
    "\n",
    "    #consolidate results\n",
    "    if combined_agg_results is None:\n",
    "        combined_agg_results = temp.copy()\n",
    "    else:\n",
    "        combined_agg_results = combined_agg_results.append(temp) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4370da-ff85-40f0-a10e-d24ddcfe6ee9",
   "metadata": {},
   "source": [
    "# Calculation of Curve Fit\n",
    "\n",
    "The previous step created a combined result data where we have a pre-aggregated and precalculated coverage for each value of the distinct aggregations keys.\n",
    "\n",
    "To use this dataset we must filter by the value of the key we want to use e.g. Large and plot the metric_coverage field. \n",
    "\n",
    "The dataset has 4 data points per metric, as we consider 4 past quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c3d12-b8ac-42da-a91f-dee98415ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Curve Fitting per Cut\n",
    "# for this to run I need to filter a) they aggregation level, b) the specific value at that aggregation level.\n",
    "# the result will be a table like key_agg, day_metric, \n",
    "\n",
    "#initialization\n",
    "combined_fitted_results = None\n",
    "\n",
    "#iterate through all the aggregated keys\n",
    "for agg_key_name in combined_agg_results.agg_key_name.unique():\n",
    "\n",
    "    #filter temporary data\n",
    "    temp_key_data = combined_agg_results[combined_agg_results['agg_key_name']==agg_key_name].copy()\n",
    "    \n",
    "    #iterate through the unique values of the specific aggregation\n",
    "    for agg_key_value in temp_key_data.agg_key_value.unique():\n",
    "        \n",
    "        # filter specific value per agg temporary data\n",
    "        temp_key_value_data = temp_key_data[temp_key_data['agg_key_value']==agg_key_value].copy()\n",
    "                \n",
    "        # set up the range of the quarter curves we want to report on\n",
    "        metrics_temp = pd.DataFrame({\"close_day_of_fiscal_quarter_normalised\":range(0,91)})\n",
    "\n",
    "        for metric_name in temp_key_value_data.metric_name.unique():\n",
    "            \n",
    "            # drop nas\n",
    "            temp_metric_data = temp_key_value_data[temp_key_value_data['metric_name']==metric_name].copy()\n",
    "            temp_metric_data.dropna(subset=['metric_coverage'], inplace=True)\n",
    "            temp_cuve = None\n",
    "            # avoid fitting curves for cuts that do not have enough data\n",
    "            if len(temp_metric_data) >= 90:     \n",
    "                temp_curve = fit_curve_to_agg (temp_metric_data,'close_day_of_fiscal_quarter_normalised','metric_coverage') \n",
    "                temp_curve.rename({'metric_coverage':metric_name+'_coverage'},inplace=1,axis=1)   \n",
    "                metrics_temp =  metrics_temp.merge(temp_curve, how='left', on='close_day_of_fiscal_quarter_normalised')                 \n",
    "            \n",
    "        # add the metric detail to the dataset\n",
    "        metrics_temp['agg_key_name'] = agg_key_name\n",
    "        metrics_temp['agg_key_value'] = agg_key_value\n",
    "        \n",
    "        #consolidate results\n",
    "        if combined_fitted_results is None:\n",
    "            combined_fitted_results = metrics_temp.copy()\n",
    "        else:\n",
    "            combined_fitted_results = combined_fitted_results.append(metrics_temp) \n",
    "        \n",
    "        # remove all nans so we can plot the charts\n",
    "        combined_fitted_results.dropna(inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8cefad-1a94-4fdf-a7cc-cbfe958036fb",
   "metadata": {},
   "source": [
    "# Linearity metric calculation - Data Extract & Fit\n",
    "\n",
    "For linearity we follow a similar process, preaggregate the metrics and then fit a curve to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54751a73-0bef-4320-8cd9-05dac918f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ data set is the only relevant for linearity calculation\n",
    "\n",
    "# columns to use for the group by (apart from the aggregation key)\n",
    "agg_columns = ['close_day_of_fiscal_quarter_normalised', 'close_fiscal_quarter_name']\n",
    "\n",
    "# fields to be summarized\n",
    "agg_fields = ['total_booked_net_arr', 'booked_net_arr']\n",
    "\n",
    "# initialize to none, add. intermediate dataframes\n",
    "# the goal is to calcualte the linearity for each aggregation key. First the data needs to be grouped\n",
    "# at that level and the relevant fields summed up. Then the coverage calculation function is used\n",
    "linearity_agg_results = None\n",
    "for agg_key in agg_key_list:\n",
    "    \n",
    "    # the key for aggregation needs to be complimented by the day key and the quarter key\n",
    "    total_agg_keys = [agg_key] + agg_columns\n",
    "\n",
    "    #calculate the aggregation per key\n",
    "    temp = cq_metrics_data.groupby(total_agg_keys,dropna=False)[agg_fields].agg('sum').reset_index().copy()\n",
    "        \n",
    "    # calculate coverages per metric, all are calculated using the \"Pending\" approach\n",
    "    # this needs to be done for every iteration \n",
    "    temp['bookings_linearity'] = temp.apply(calculate_bookings_linearity, axis=1,qtd_bookings='booked_net_arr',actual_booked='total_booked_net_arr')\n",
    "\n",
    "    # clean up / add information regarding the agg key\n",
    "    temp = temp.rename({agg_key:'agg_key_value'},axis=1)\n",
    "    temp['agg_key_name'] = agg_key\n",
    "\n",
    "    #consolidate results\n",
    "    if linearity_agg_results is None:\n",
    "        linearity_agg_results = temp.copy()\n",
    "    else:\n",
    "        linearity_agg_results = linearity_agg_results.append(temp) \n",
    "\n",
    "# set linearity 0 to NaN\n",
    "linearity_agg_results[linearity_agg_results['bookings_linearity']==0]=np.NaN\n",
    "\n",
    "# remove NaNs from dataset\n",
    "linearity_agg_results.dropna(subset=['bookings_linearity'],axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ca4c1-c26b-459f-b335-9d7184b66a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Curve Fitting per Cut\n",
    "# for this to run I need to filter a) they aggregation level, b) the specific value at that aggregation level.\n",
    "# the result will be a table like key_agg, day_metric, \n",
    "\n",
    "#initialization\n",
    "linearity_fitted_results = None\n",
    "\n",
    "#iterate through all the aggregated keys\n",
    "for agg_key_name in linearity_agg_results.agg_key_name.unique():\n",
    "\n",
    "    #filter temporary data\n",
    "    temp_key_data = linearity_agg_results[linearity_agg_results['agg_key_name']==agg_key_name].copy()\n",
    "    linearity_temp = None\n",
    "    \n",
    "    #iterate through the unique values of the specific aggregation\n",
    "    for agg_key_value in temp_key_data.agg_key_value.unique():\n",
    "        \n",
    "        # filter specific value per agg temporary data\n",
    "        temp_key_value_data = temp_key_data[temp_key_data['agg_key_value']==agg_key_value].copy()\n",
    "                \n",
    "        # set up the range of the quarter curves we want to report on\n",
    "        linearity_temp = pd.DataFrame({\"close_day_of_fiscal_quarter_normalised\":range(0,91)})\n",
    "                 \n",
    "        # drop nas\n",
    "        temp_linearity_data = temp_key_value_data.copy()\n",
    "        temp_cuve = None\n",
    "\n",
    "        # avoid fitting curves for cuts that do not have enough data\n",
    "        if len(temp_linearity_data) > 180:     \n",
    "            temp_curve = fit_curve_to_agg (temp_linearity_data,'close_day_of_fiscal_quarter_normalised','bookings_linearity') \n",
    "            linearity_temp =  linearity_temp.merge(temp_curve, how='left', on='close_day_of_fiscal_quarter_normalised')                 \n",
    "            \n",
    "        # add the metric detail to the dataset\n",
    "        linearity_temp['agg_key_name'] = agg_key_name\n",
    "        linearity_temp['agg_key_value'] = agg_key_value\n",
    "        \n",
    "        #consolidate results\n",
    "        if linearity_fitted_results is None:\n",
    "            linearity_fitted_results = linearity_temp.copy()\n",
    "        else:\n",
    "            linearity_fitted_results = linearity_fitted_results.append(linearity_temp) \n",
    "        \n",
    "# remove all nans so we can plot the charts\n",
    "linearity_fitted_results.dropna(subset=['bookings_linearity'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9ca18-808a-4546-b610-828d115622ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine with the coverage results\n",
    "combined_fitted_results = combined_fitted_results.merge(linearity_fitted_results, how='left',on=['close_day_of_fiscal_quarter_normalised','agg_key_name','agg_key_value'])\n",
    "combined_fitted_results.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18e64e-51ed-4825-8e6c-b7a2da03f2db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save to Gsheets\n",
    "\n",
    "The final goal is to get this process to run automatically once a quarter and save the results into a Snowflake database. Until that works, we save the curve results into a tab in the Source X-Ray file.\n",
    "\n",
    "Link to the file X-Ray Source file:\n",
    "\n",
    "- https://docs.google.com/spreadsheets/d/1Vwu8euxRgIF3QYWK8hAbp4Vy21AlFfpDwI4MaEEiIWk/edit#gid=930691697\n",
    "\n",
    "Link to the sheetload file:\n",
    "- https://docs.google.com/spreadsheets/d/1dLevdYA8QMjpIV9irNGD8KTfKIgc3xJdrRDA2vH7M50/edit#gid=856928614\n",
    "\n",
    "For the automatic upload on using DRIVE load from the Data Team, the column names MUST BE UPPERCASE.\n",
    "\n",
    "Upload issue: https://gitlab.com/gitlab-data/analytics/-/issues/12157\n",
    "\n",
    "The target folders are: \n",
    "\n",
    "- SS&A Coverage Fit\n",
    "https://drive.google.com/drive/folders/1jfIPIbYrNO7ApTphG_0qG-IkRVvpBn3W\n",
    "\n",
    "- SS&A Metric Data (Pre-Aggregated)\n",
    "https://drive.google.com/drive/folders/19H3G2gaIBmYppDiqRxB_ChCcGx4OIZeP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b22c31-c931-410e-8b2b-4c61208fb7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/59815620/gcloud-upload-httplib2-redirectmissinglocation-redirected-but-the-response-is-m\n",
    "!pip install httplib2==0.15.0\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "scope = [\"https://www.googleapis.com/auth/drive\"]\n",
    "gauth.credentials = ServiceAccountCredentials.from_json_keyfile_name(service_file_path, scope)\n",
    "drive = GoogleDrive(gauth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793fc2c-c503-4503-9700-571418acba33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Raw data used to calculate the fitted curves\n",
    "sheet_name = 'ssa_quarterly_aggregated_metrics_for_coverage'\n",
    "\n",
    "# set the agg_key_value to string so we can index jihu out\n",
    "\n",
    "combined_agg_results = combined_agg_results[index].dropna(subset=['agg_key_value'], how='all').copy()\n",
    "combined_agg_results['agg_key_value'] = combined_agg_results['agg_key_value'].astype(str)\n",
    "subset = ['agg_key_value','metric_value', 'total_booked_net_arr', 'booked_net_arr','metric_coverage']\n",
    "index = ~combined_agg_results['agg_key_value'].str.contains('jihu')\n",
    "for_gsheet = combined_agg_results[index].dropna(subset=subset, how='all').copy()\n",
    "\n",
    "# nans are not welcome in the drive upload process\n",
    "index = ~for_gsheet['agg_key_value'].str.contains('na')\n",
    "for_gsheet = for_gsheet[index].copy()\n",
    "\n",
    "#columns need to be upper case for the drive upload process to work\n",
    "for_gsheet.columns = for_gsheet.columns.str.upper()\n",
    "fields_order = ['AGG_KEY_VALUE', 'METRIC_NAME', 'CLOSE_DAY_OF_FISCAL_QUARTER_NORMALISED', 'CLOSE_FISCAL_QUARTER_NAME', 'METRIC_VALUE', 'TOTAL_BOOKED_NET_ARR', 'BOOKED_NET_ARR', 'METRIC_COVERAGE', 'AGG_KEY_NAME']\n",
    "for_gsheet = for_gsheet[fields_order].copy()\n",
    "\n",
    "# store to CSV\n",
    "for_gsheet.to_csv('ssa_quarterly_aggregated_metrics_for_coverage_V2.csv',index=False) #ssa_quarterly_aggregated_metrics_for_coverage.csv\n",
    "\n",
    "# upload to drive folder for autoupload -> FILE IS TO BIG FOR THIS TO WORK\n",
    "#https://drive.google.com/file/d/13mjP-A831mTMlNWXNOA5EfRm_uoI5_Oo/view?usp=sharing\n",
    "gfile = drive.CreateFile({'parents': [{'id': '19H3G2gaIBmYppDiqRxB_ChCcGx4OIZeP'}]})\n",
    "gfile.SetContentFile('ssa_quarterly_aggregated_metrics_for_coverage_V2.csv')\n",
    "gfile.Upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b563e-8334-4264-9102-7b4a852dd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to the X-Ray source file, this is temporary while we do not have the data in the database\n",
    "sheet_name = 'hist_coverage_fitted_curves'\n",
    "\n",
    "subset = ['open_1plus_net_arr_coverage', 'open_3plus_net_arr_coverage','open_4plus_net_arr_coverage','rq_plus_1_open_1plus_net_arr_coverage','rq_plus_1_open_3plus_net_arr_coverage','rq_plus_1_open_4plus_net_arr_coverage', 'rq_plus_2_open_1plus_net_arr_coverage','rq_plus_2_open_3plus_net_arr_coverage','rq_plus_2_open_4plus_net_arr_coverage']\n",
    "index = ~combined_fitted_results['agg_key_value'].str.contains('jihu')\n",
    "for_gsheet = combined_fitted_results[index].dropna(subset=subset, how='all').copy()\n",
    "\n",
    "for_gsheet['key_agg_day'] = for_gsheet['agg_key_value'] + '_' + for_gsheet['close_day_of_fiscal_quarter_normalised'].astype(str) \n",
    "fields_order = ['key_agg_day','agg_key_name','agg_key_value','close_day_of_fiscal_quarter_normalised','bookings_linearity','open_1plus_net_arr_coverage', 'open_3plus_net_arr_coverage','open_4plus_net_arr_coverage','rq_plus_1_open_1plus_net_arr_coverage','rq_plus_1_open_3plus_net_arr_coverage','rq_plus_1_open_4plus_net_arr_coverage', 'rq_plus_2_open_1plus_net_arr_coverage','rq_plus_2_open_3plus_net_arr_coverage','rq_plus_2_open_4plus_net_arr_coverage']\n",
    "\n",
    "# columns in the CSV MUST BE UPPERCASE, if not, the drive process might fail\n",
    "for_gsheet = for_gsheet[fields_order].copy()\n",
    "for_gsheet.columns = for_gsheet.columns.str.upper()\n",
    "\n",
    "write_to_gsheet(service_file_path, curves_sheetload_spreadsheet_id, sheet_name,for_gsheet)\n",
    "\n",
    "# Store data in CSV\n",
    "for_gsheet.to_csv('ssa_coverage_fitted_curves.csv',index=False)\n",
    "\n",
    "# upload to drive folder for autoupload\n",
    "# file link https://drive.google.com/file/d/1jM_ur0gVnhNjzQZobFfwWHGnS1jujnJg/view?usp=sharing\n",
    "\n",
    "gfile = drive.CreateFile({'id':'1jM_ur0gVnhNjzQZobFfwWHGnS1jujnJg','parents': [{'id': '1jfIPIbYrNO7ApTphG_0qG-IkRVvpBn3W'}]})\n",
    "gfile.SetContentFile('ssa_coverage_fitted_curves.csv')\n",
    "gfile.Upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d5aea-2eda-4475-b3aa-d78474e89f1e",
   "metadata": {},
   "source": [
    "# Test Code\n",
    "\n",
    "The following snippets were used during development to test the approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58504ec-047c-432a-9b73-f78e89e407b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that results make sense\n",
    "cq_plus2_metrics_data.groupby(['close_fiscal_quarter_name','close_day_of_fiscal_quarter_normalised'])['rq_plus_2_open_1plus_net_arr'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21acac-bc99-4f93-8e77-3a700c2823ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Test that the totals work. E.g. FY22Q4 Large = 29043057\n",
    "# combined_df[(combined_df['close_day_of_fiscal_quarter_normalised']==40) & (combined_df['metric']=='open_1plus_net_arr')].groupby(by=['key_segment','close_fiscal_quarter_name']).total_booked_net_arr.sum()\n",
    "\n",
    "####### Test that the coverage curve makes sense for one metric\n",
    "index = (combined_df['key_segment']=='large') & (combined_df['metric_name']=='open_1plus_net_arr') & (combined_df['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_df[index].groupby(['close_day_of_fiscal_quarter_normalised','key_segment']).agg({'total_booked_net_arr':'sum','metric_value':'sum','booked_net_arr':'sum'}).reset_index()\n",
    "\n",
    "# calculate coverages per metric, all are calculated using the \"Pending\" approach\n",
    "# this needs to be done for every iteration \n",
    "temp['metric_coverage'] = temp.apply(calculate_to_pending_coverage, axis=1,metric='metric_value',qtd='booked_net_arr',actual='total_booked_net_arr')\n",
    "\n",
    "sns.lineplot(data=temp,hue='key_segment', y='metric_coverage',x='close_day_of_fiscal_quarter_normalised')\n",
    "\n",
    "#### Test #1 that combined pre-grouped dataset has reasonable values\n",
    "combined_agg_results.head()\n",
    "index = (combined_agg_results['agg_key_value']=='large') & (combined_agg_results['metric_name']=='open_1plus_net_arr') & (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_agg_results[index]\n",
    "temp.head()\n",
    "sns.lineplot(data=temp,hue='agg_key_value', y='metric_coverage',x='close_day_of_fiscal_quarter_normalised')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c7a4b-f314-409d-8f1c-dd736fcf4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Test #2 that combined pre-grouped dataset has reasonable values\n",
    "\n",
    "#key_value_filter = 'large_apac_apac_korea_2. growth'\n",
    "key_value_filter = 'large'\n",
    "metric_value_filter = 'rq_plus_2_open_1plus_net_arr'\n",
    "\n",
    "combined_agg_results.head()\n",
    "index = (combined_agg_results['agg_key_value']==key_value_filter) & (combined_agg_results['metric_name']==metric_value_filter) #& (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_agg_results[index]\n",
    "sns.lineplot(data=temp,hue='close_fiscal_quarter_name', y='metric_coverage',x='close_day_of_fiscal_quarter_normalised')\n",
    "index = (combined_fitted_results['agg_key_value']==key_value_filter) #& (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_fitted_results[index]\n",
    "sns.lineplot(data=temp, y=metric_value_filter + '_coverage',x='close_day_of_fiscal_quarter_normalised',color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e6fc73-d255-4a16-aeea-f40245a85be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test #2a that combined pre-grouped dataset has reasonable values\n",
    "\n",
    "key_value_filter = 'large_apac_apac_korea_2. growth'\n",
    "metric_value_filter = 'bookings_linearity'\n",
    "\n",
    "combined_agg_results.head()\n",
    "index = (combined_agg_results['agg_key_value']==key_value_filter) & (combined_agg_results['metric_name']==metric_value_filter) #& (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_agg_results[index]\n",
    "sns.lineplot(data=temp,hue='close_fiscal_quarter_name', y='metric_coverage',x='close_day_of_fiscal_quarter_normalised')\n",
    "combined_fitted_results.head()\n",
    "index = (combined_fitted_results['agg_key_value']==key_value_filter) #& (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_fitted_results[index]\n",
    "sns.lineplot(data=temp, y=metric_value_filter,x='close_day_of_fiscal_quarter_normalised',color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa111d5-dd34-4d98-943c-3733048a4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test #3 that combined pre-grouped dataset has reasonable values\n",
    "combined_fitted_results.head()\n",
    "index = (combined_fitted_results['agg_key_value']=='large_amer_west')\n",
    "temp = combined_fitted_results[index]\n",
    "temp.head()\n",
    "sns.lineplot(data=temp,hue='agg_key_value', y='open_1plus_net_arr_coverage',x='close_day_of_fiscal_quarter_normalised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f14e7a-3489-47b0-9fa4-eab2ea400c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fitted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c940b9d-12da-48ff-9cc2-e45ab737d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_agg_results[combined_agg_results['agg_key_value']=='pubsec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee30652-8f10-41c6-84a6-110b3d3486a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_fitted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf255a0-84ac-4acf-9a37-52b70cc9dc79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
