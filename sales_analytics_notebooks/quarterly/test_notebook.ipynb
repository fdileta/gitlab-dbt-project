{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305aeda-ad47-48bd-8129-1c4946b19948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for the ETL run\n",
    "\n",
    "# Set to true when trying to refresh the coverage curves on the Database\n",
    "#  it is only needed once per quarter.\n",
    "upload_to_drive_csv = True \n",
    "\n",
    "# Set it to true to store the results of the coverage fitting process in the target\n",
    "# gsheet\n",
    "save_to_gsheets_and_csv = True\n",
    "\n",
    "\n",
    "# Reload data from database\n",
    "# if set to true the etl will get new data from snowflake.\n",
    "# if false, the notebook will read a local csv file\n",
    "load_db_data = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd0edb-b4c3-4d0e-9a9a-4914dea966cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os as os\n",
    "\n",
    "reqs = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze'])\n",
    "installed_packages = [r.decode().split('==')[0] for r in reqs.split()]\n",
    "\n",
    "# only run if packages are not installed\n",
    "if 'pygsheets' not in installed_packages:\n",
    "    !{sys.executable} -m pip install  pygsheets\n",
    "    !{sys.executable} -m pip install \"pyarrow<5.1.0,>=5.0.0;\"\n",
    "    !{sys.executable} -m pip install  pydrive\n",
    "\n",
    "import pygsheets\n",
    "import configparser\n",
    "import sys\n",
    "import snowflake.connector\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "import numpy as np                  \n",
    "import matplotlib.pyplot as plt     \n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from math import floor \n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323bd6be-1a82-4f84-ad61-83c29ff022a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set base permission files and snowflake engine\n",
    "\n",
    "# link to snowflake library\n",
    "from gitlabdata.orchestration_utils import (\n",
    "    data_science_engine_factory,\n",
    "    query_dataframe,\n",
    ")\n",
    "\n",
    "engine = data_science_engine_factory()\n",
    "\n",
    "\n",
    "#target gsheet for uploads and downloads, just allowing one to keep it more secure\n",
    "# x-ray source target id\n",
    "x_ray_spreadsheet_id = '1Vwu8euxRgIF3QYWK8hAbp4Vy21AlFfpDwI4MaEEiIWk'\n",
    "\n",
    "# the following gsheets are used to push the data into the snowflake\n",
    "curves_sheetload_spreadsheet_id = '1dLevdYA8QMjpIV9irNGD8KTfKIgc3xJdrRDA2vH7M50'\n",
    "\n",
    "#key to be able to work with bigquery / gsheets\n",
    "service_file_path = '/Users/nfiguera/repos/sales-strategy-and-analytics-business-intelligence/202202_fy22_channel_value_analysis/nfiguera-c3fe9e64-a6543dd51e79.json'\n",
    "\n",
    "\n",
    "os.getcwd()\n",
    "\n",
    "# NF: Just to deal with my working directory changing\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "os.chdir(\"/Users/nfiguera/repos/sales-strategy-and-analytics-business-intelligence/202202_targets_&_coverage_curves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f91ac-f205-43d4-a2e7-54fc34ea11c4",
   "metadata": {},
   "source": [
    "# Model Description:\n",
    "\n",
    "## Goal\n",
    "\n",
    "Retrieve the quarterly coverage raw data & calculate fitted coverage curves for each level of aggregation\n",
    " \n",
    "## Steps\n",
    "\n",
    "- Retrieve data metrics per day models\n",
    "- Calculate the quarters needed for each metric\n",
    "- Integrate the different cuts into a single tall table\n",
    "- Pre-aggregate the data by business cut & calculate coverage\n",
    "- Using pre-aggregated data, fit a curve for each pre-aggregated model\n",
    "- Push changes to gSheet\n",
    "\n",
    "## Challenges with automation\n",
    "\n",
    "- How can we authenticate on gSheets without using my own auth json. (Maybe a service account within the library)?\n",
    "- How can we push data into Snowflake, ideally the workspace_sales directly from the jupyter workbook?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa37ee-913a-4bc3-b638-12c1841ed286",
   "metadata": {},
   "source": [
    "## Update to FY / 4Q historical curves\n",
    "\n",
    "List of steps:\n",
    "- Create metric per day for FY\n",
    "- How to create a metric day perspective for 4Q cuts?\n",
    "  - Starting quarter and end quarter\n",
    "  - Total booked on that time period\n",
    "  - Day 365 is the last day of end quarter\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014121ff-57ae-45c7-807a-a74f74ae0505",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "import pydrive\n",
    "\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "gauth = pydrive.auth.GoogleAuth(service_file_path).LoadClientConfigFile(service_file_path)\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'https://drive.google.com/file/d/1RIuSxxpd3Q_2bfpxPng5fu3Oaa3wqC0j'}) # replace the id with id of the file you want to access\n",
    "downloaded.GetContentFile('file.csv')  \n",
    "GoogleAuth(service_file_path)\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'https://drive.google.com/file/d/1RIuSxxpd3Q_2bfpxPng5fu3Oaa3wqC0j'}) # replace the id with id of the file you want to access\n",
    "downloaded.GetContentFile('file.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf61a5c-bfb1-4468-bc34-9b1af47895c4",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5a9314-f84e-48df-87cd-52797f727f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#######\n",
    "## Create a fitter curve of the last 4 quarters for comparison\n",
    "#######\n",
    "def objective(x, a, b, c, d, e):\n",
    "    return a * x + b * x**2 + c * x**3 + d * x**4 + e\n",
    "\n",
    "\n",
    "def write_to_gsheet(service_file_path, spreadsheet_id, sheet_name, data_df):\n",
    "    \"\"\"\n",
    "    this function takes data_df and writes it under spreadsheet_id\n",
    "    and sheet_name using your credentials under service_file_path\n",
    "    \"\"\"\n",
    "    gc = pygsheets.authorize(service_file=service_file_path)\n",
    "    sh = gc.open_by_key(spreadsheet_id)\n",
    "    try:\n",
    "        sh.add_worksheet(sheet_name)\n",
    "    except:\n",
    "        pass\n",
    "    wks_write = sh.worksheet_by_title(sheet_name)\n",
    "    wks_write.clear('A1',None,'*')\n",
    "    wks_write.set_dataframe(data_df, (1,1), encoding='utf-8', fit=True)\n",
    "    wks_write.frozen_rows = 1\n",
    "\n",
    "def read_from_gsheet(service_file_path, spreadsheet_id, sheet_name):\n",
    "    \"\"\"\n",
    "    this function takes a sheet_name from a spreadsheet_id and returns a data frame \n",
    "    \"\"\"\n",
    "    gc = pygsheets.authorize(service_file=service_file_path)\n",
    "    sh = gc.open_by_key(spreadsheet_id)\n",
    "   \n",
    "    wks_read = sh.worksheet_by_title(sheet_name)\n",
    "    read = wks_read.get_as_df()\n",
    "    \n",
    "    return read\n",
    "\n",
    "\n",
    "def run_query_in_snowflake(conn, sql):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    df = cur.fetch_pandas_all()\n",
    "    return df\n",
    "\n",
    "def executeScriptFromFile(filename, engine):\n",
    "    # Open and read the file as a single buffer\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "    \n",
    "    print(filename)\n",
    "    print(len(sqlFile))\n",
    "    \n",
    "    results = -1\n",
    "    \n",
    "    try:\n",
    "        results = query_dataframe(engine,sqlFile)\n",
    "    except:\n",
    "        print(\"Command did not run\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def days_between(d1, d2):\n",
    "    #d1 = datetime.strptime(d1, \"%Y-%m-%d\")\n",
    "    #d2 = datetime.strptime(d2, \"%Y-%m-%d\")\n",
    "    return (d2 - d1).days\n",
    "\n",
    "\n",
    "def calculate_quarters_after_creation(x):\n",
    "        \n",
    "    age = 0\n",
    "    \n",
    "    if (x['IS_OPEN'] == 1):\n",
    "        age = days_between(x['CREATED_FISCAL_QUARTER_DATE'], x['SNAPSHOT_FISCAL_QUARTER_DATE'])\n",
    "    elif (x['IS_OPEN']== 0 and x['SNAPSHOT_DATE'] <= x['CLOSE_DATE']):\n",
    "        age = days_between(x['CREATED_FISCAL_QUARTER_DATE'], x['SNAPSHOT_FISCAL_QUARTER_DATE'])\n",
    "    else:\n",
    "        age = days_between(x['CREATED_FISCAL_QUARTER_DATE'], x['CLOSE_FISCAL_QUARTER_DATE'])\n",
    "    \n",
    "    quarter_delta = floor(age/90)\n",
    "    \n",
    "    return quarter_delta\n",
    "\n",
    "def calculate_channel_track (x):\n",
    "    \n",
    "    channel_track = 'Direct'\n",
    "    \n",
    "    if (x['deal_path'] == 'Direct'):\n",
    "        channel_track = 'Direct'\n",
    "    elif (x['deal_path'] == 'Web Direct'):\n",
    "        channel_track = 'Web Direct'\n",
    "    elif (x['deal_path'] == 'Channel'\n",
    "        and x['sales_qualified_source'] != 'Channel Generated'): \n",
    "        channel_track = 'Partner Co-Sell'\n",
    "    elif (x['deal_path'] == 'Channel'):\n",
    "        channel_track = 'Partner Sourced'\n",
    "    \n",
    "    return channel_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177769ee-7fbd-4d1a-a841-79b9e72cefb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculates pending coverage using a minimum of 50k pending\n",
    "def calculate_to_pending_coverage (x, metric, qtd, actual, minimum_delta=5000):\n",
    "    \n",
    "    result = None\n",
    "    \n",
    "    actual = float(x[actual])\n",
    "    qtd = float(x[qtd])\n",
    "    metric = float(x[metric])\n",
    "    \n",
    "    if (actual - qtd) > minimum_delta:\n",
    "        result = metric / (actual - qtd)\n",
    "        result = min(result,6) # limiting the maximum amount of coverage to account for noise in the models\n",
    "    \n",
    "    return result\n",
    "\n",
    "# calculates pending coverage using a minimum of 50k pending\n",
    "def calculate_bookings_linearity (x, qtd_bookings, actual_booked):\n",
    "    \n",
    "    result = None\n",
    "    \n",
    "    actual = float(x[actual_booked])\n",
    "    qtd = float(x[qtd_bookings])\n",
    "       \n",
    "    if actual > 0:\n",
    "        result = qtd / actual \n",
    "        \n",
    "    return result \n",
    "\n",
    "\n",
    "# fits a curve to the subset data using the defined objective function\n",
    "def fit_curve_to_agg (data_agg, x_label, y_label):\n",
    "    \n",
    "    # fit a curve\n",
    "    x, y = data_agg[x_label], data_agg[y_label]\n",
    "    # curve fit\n",
    "    popt, _ = curve_fit(objective, x, y, method='dogbox')\n",
    "\n",
    "    x_line = np.arange(min(x), max(x), 1)\n",
    "    # calculate the output for the range\n",
    "    # summarize the parameter values\n",
    "    a, b, c , d, e = popt\n",
    "    y_line = objective(x_line, a, b, c, d, e)\n",
    "  \n",
    "    curve_result = pd.DataFrame({x_label:x_line,y_label:y_line})\n",
    "    return curve_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341e97b-d374-4d3d-8c90-da0b71a592b0",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce9e33-bb56-4aac-8f41-685291774950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### considered keys\n",
    "agg_key_list = ['key_segment','key_overall','sales_team_rd_asm_level',\n",
    "       'key_sqs', 'key_ot','key_segment_sqs', 'key_segment_ot', 'key_segment_geo', 'key_segment_geo_sqs',\n",
    "       'key_segment_geo_ot', 'key_segment_geo_region',\n",
    "       'key_segment_geo_region_sqs', 'key_segment_geo_region_ot',\n",
    "       'key_segment_geo_region_area', 'key_segment_geo_region_area_sqs','key_ent_comm',\n",
    "       'key_segment_geo_region_area_ot','report_user_segment_geo_region_area'] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269a271-f2e3-4922-94c8-d93e1e23de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect quarterly data\n",
    "sql_file = 'qtr_metrics_by_day'\n",
    "\n",
    "#load_db_data = False\n",
    "\n",
    "if load_db_data: \n",
    "    print (\"Executing {} Snowflake SQL\".format(sql_file))\n",
    "    qtr_metrics_by_day = executeScriptFromFile(sql_file+'.sql', engine)\n",
    "    qtr_metrics_by_day.to_csv(sql_file+'.csv',index=False)\n",
    "else:\n",
    "    print (\"Reading Local CSV {}.csv\".format(sql_file))\n",
    "    qtr_metrics_by_day = pd.read_csv(sql_file+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ad23d-3f95-4446-ab0d-2958c1199ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted quarters\n",
    "qtr_metrics_by_day.close_fiscal_quarter_name.sort_values().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157c4a2-f8d7-4bd4-8e42-9c1c9987d98c",
   "metadata": {},
   "source": [
    "# Data Transformation\n",
    "\n",
    "We collect the data and identify the quarters that we need for each metric.\n",
    "\n",
    "The way the model is construct, it will have for any given quarter values for open pipeline closing in the same quarter and in future quarters.\n",
    "\n",
    "To calculate the Current Quarter + 1 & + 2 metrics, we use the specific future field and filter the close quarter to the right point in time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887eba8-4eb1-408b-828a-ae209a9df4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform potential strings to date (in case of data read from CSV)\n",
    "qtr_metrics_by_day['close_fiscal_quarter_date'] = pd.to_datetime(qtr_metrics_by_day['close_fiscal_quarter_date']).dt.date\n",
    "qtr_metrics_by_day['rq_plus_1_close_fiscal_quarter_date'] = pd.to_datetime(qtr_metrics_by_day['rq_plus_1_close_fiscal_quarter_date']).dt.date\n",
    "qtr_metrics_by_day['rq_plus_2_close_fiscal_quarter_date'] = pd.to_datetime(qtr_metrics_by_day['rq_plus_2_close_fiscal_quarter_date']).dt.date\n",
    "\n",
    "#################\n",
    "##### Data Injections\n",
    "\n",
    "# create an artificial global key to calculate a global curve for the whole company\n",
    "qtr_metrics_by_day['key_overall'] = 'global'\n",
    "\n",
    "# adjust the sales_team legacy key to be sure that it won't generate duplicate with the FY23 key logic\n",
    "qtr_metrics_by_day['sales_team_rd_asm_level'] = 'st_rd_' + qtr_metrics_by_day['sales_team_rd_asm_level']\n",
    "\n",
    "# add a commercial and enterprise consolidated key\n",
    "#add key enterprise commercial for x-ray reporting\n",
    "qtr_metrics_by_day['key_ent_comm'] = 'other'\n",
    "qtr_metrics_by_day.loc[qtr_metrics_by_day['key_segment']=='large','key_ent_comm'] = 'enterprise'\n",
    "qtr_metrics_by_day.loc[qtr_metrics_by_day['key_segment']=='pubsec','key_ent_comm'] = 'enterprise'\n",
    "qtr_metrics_by_day.loc[qtr_metrics_by_day['key_segment']=='mid-market','key_ent_comm'] = 'commercial'\n",
    "qtr_metrics_by_day.loc[qtr_metrics_by_day['key_segment']=='smb','key_ent_comm'] = 'commercial'\n",
    "\n",
    "#################\n",
    "\n",
    "# identify the quarters that will be considered when creating the curves\n",
    "# for current quarter last 4 quarters\n",
    "index_cond = (qtr_metrics_by_day['close_fiscal_quarter_date'] <= date.today() + relativedelta(months=-3)) & (qtr_metrics_by_day['close_fiscal_quarter_date'] >= date.today() + relativedelta(months=-15))\n",
    "cq_considered_quarters = qtr_metrics_by_day[index_cond].close_fiscal_quarter_name.sort_values().unique()\n",
    "\n",
    "# for current quarter + 1, between 5 and 1 quarter ago (as we need the total won amount of the quarter to calculate coverage)\n",
    "index_cond = (qtr_metrics_by_day['close_fiscal_quarter_date'] <= date.today() + relativedelta(months=-6)) & (qtr_metrics_by_day['close_fiscal_quarter_date'] >= date.today() + relativedelta(months=-18))\n",
    "cq_1plus_considered_quarters = qtr_metrics_by_day[index_cond].close_fiscal_quarter_name.sort_values().unique()\n",
    "\n",
    "# for current quarter + 2, between 6 and 2 quarter ago (as we need the total won amount of the quarter to calculate coverage)\n",
    "index_cond = (qtr_metrics_by_day['close_fiscal_quarter_date'] <= date.today() + relativedelta(months=-9)) & (qtr_metrics_by_day['close_fiscal_quarter_date'] >= date.today() + relativedelta(months=-21))\n",
    "cq_2plus_considered_quarters = qtr_metrics_by_day[index_cond].close_fiscal_quarter_name.sort_values().unique()\n",
    "\n",
    "print(cq_considered_quarters)\n",
    "print(cq_1plus_considered_quarters)\n",
    "print(cq_2plus_considered_quarters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701a9f8-befd-4b22-b44b-48b97fd47b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate considered data points per metric type\n",
    "cq_metrics_data = qtr_metrics_by_day[qtr_metrics_by_day['close_fiscal_quarter_name'].isin(cq_considered_quarters)].copy()\n",
    "cq_plus1_metrics_data = qtr_metrics_by_day[qtr_metrics_by_day['close_fiscal_quarter_name'].isin(cq_1plus_considered_quarters)].copy()\n",
    "cq_plus2_metrics_data = qtr_metrics_by_day[qtr_metrics_by_day['close_fiscal_quarter_name'].isin(cq_2plus_considered_quarters)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b024bc-9274-481e-af28-fe5815c61b34",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## From a wide table to a tall table\n",
    "\n",
    "After selecting and appending the relevant datasets into a single data with the same column name structure we unpivot the table into a tall one with a line per metric per day per fiscal quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f213b3-9116-4e5c-85b5-3161782ae98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are three fields that are critical\n",
    "# - Open Pipe Net ARR (Stage 1, 3, 4) (These can be metrics in a tall table)\n",
    "# - Booked Amount (This could be a column)\n",
    "# - Total Booked Amount (This can also be a column)\n",
    "\n",
    "\n",
    "### current quarter\n",
    "variables = ['close_day_of_fiscal_quarter_normalised','close_fiscal_quarter_name','total_booked_net_arr','booked_net_arr'] + agg_key_list\n",
    "considered_metrics = ['open_1plus_net_arr','open_3plus_net_arr','open_4plus_net_arr'] \n",
    "cq_melt = cq_metrics_data.melt(id_vars = variables, value_vars = considered_metrics)\n",
    "\n",
    "### current quarter + 1 \n",
    "variables = ['close_day_of_fiscal_quarter_normalised','close_fiscal_quarter_name','rq_plus_1_total_booked_net_arr','booked_net_arr'] + agg_key_list \n",
    "considered_metrics = ['rq_plus_1_open_1plus_net_arr', 'rq_plus_1_open_3plus_net_arr',\n",
    "       'rq_plus_1_open_4plus_net_arr']\n",
    "cq_plus1_melt = cq_plus1_metrics_data.melt(id_vars = variables, value_vars = considered_metrics)\n",
    "# rename the total booked field to keep it consistent\n",
    "cq_plus1_melt = cq_plus1_melt.rename({'rq_plus_1_total_booked_net_arr':'total_booked_net_arr'}, axis=1)\n",
    "cq_plus1_melt['booked_net_arr'] = 0\n",
    "\n",
    "### current quarter + 2\n",
    "variables = ['close_day_of_fiscal_quarter_normalised','close_fiscal_quarter_name','rq_plus_2_total_booked_net_arr','booked_net_arr'] + agg_key_list\n",
    "considered_metrics = ['rq_plus_2_open_1plus_net_arr', 'rq_plus_2_open_3plus_net_arr',\n",
    "       'rq_plus_2_open_4plus_net_arr']\n",
    "cq_plus2_melt = cq_plus2_metrics_data.melt(id_vars = variables, value_vars = considered_metrics)\n",
    "# rename the total booked field to keep it consistent\n",
    "cq_plus2_melt = cq_plus2_melt.rename({'rq_plus_2_total_booked_net_arr':'total_booked_net_arr'}, axis=1)\n",
    "cq_plus2_melt['booked_net_arr'] = 0\n",
    "\n",
    "### consolidated dataset\n",
    "combined_df = pd.concat([cq_melt, cq_plus1_melt,cq_plus2_melt], ignore_index=True)\n",
    "combined_df = combined_df.rename({'variable':'metric_name'}, axis=1)\n",
    "combined_df = combined_df.rename({'value':'metric_value'}, axis=1)\n",
    "\n",
    "##########################################################\n",
    "# to be able to calculate coverage, we need to drop:\n",
    "# - Lines where total booked net arr = 0\n",
    "# - Lines where value is NaN or 0\n",
    "# - Lines where booked_net_arr is NaN are to be set to 0\n",
    "\n",
    "###\n",
    "# - Lines where total booked net arr = 0\n",
    "# - Lines where value is NaN or 0\n",
    "combined_df.dropna(subset=['total_booked_net_arr', 'metric_value'], inplace=True)\n",
    "\n",
    "# - Lines where booked_net_arr is NaN are to be set to 0\n",
    "# combined_df = combined_df.drop(combined_df[combined_df['total_booked_net_arr']==0].index)\n",
    "len(combined_df)\n",
    "\n",
    "# test result\n",
    "print(len(combined_df), ' # of rows - ', combined_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34899261-4d19-454b-8c62-6edeba7734b6",
   "metadata": {},
   "source": [
    "## Calculation of coverages per Business cut\n",
    "\n",
    "For each business cut we need to do an aggregation of the relevant fields and from there run the coverage calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854577d-271f-47d4-a813-904130920662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation columns\n",
    "agg_columns = ['metric_name','close_day_of_fiscal_quarter_normalised', 'close_fiscal_quarter_name']\n",
    "# fields to be summarized\n",
    "agg_fields = ['metric_value','total_booked_net_arr', 'booked_net_arr']\n",
    "\n",
    "# initialize to none, add. intermediate dataframes\n",
    "# the goal is to calcualte the coverage for each aggregation key. First the data needs to be grouped\n",
    "# at that level and the relevant fields summed up. Then the coverage calculation function is used\n",
    "combined_agg_results = None\n",
    "for agg_key in agg_key_list:\n",
    "    \n",
    "    # the key for aggregation needs to be complimented by the day key and the quarter key\n",
    "    total_agg_keys = [agg_key] + agg_columns\n",
    "\n",
    "    #calculate the aggregation per key\n",
    "    temp = combined_df.groupby(total_agg_keys,dropna=False)[agg_fields].agg('sum').reset_index().copy()\n",
    "        \n",
    "    # calculate coverages per metric, all are calculated using the \"Pending\" approach\n",
    "    # this needs to be done for every iteration \n",
    "    temp['metric_coverage'] = temp.apply(calculate_to_pending_coverage, axis=1,metric='metric_value',qtd='booked_net_arr',actual='total_booked_net_arr')\n",
    "\n",
    "    # clean up / add information regarding the agg key\n",
    "    temp = temp.rename({agg_key:'agg_key_value'},axis=1)\n",
    "    temp['agg_key_name'] = agg_key\n",
    "\n",
    "    #consolidate results\n",
    "    if combined_agg_results is None:\n",
    "        combined_agg_results = temp.copy()\n",
    "    else:\n",
    "        combined_agg_results = combined_agg_results.append(temp) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4370da-ff85-40f0-a10e-d24ddcfe6ee9",
   "metadata": {},
   "source": [
    "# Calculation of Curve Fit\n",
    "\n",
    "The previous step created a combined result data where we have a pre-aggregated and precalculated coverage for each value of the distinct aggregations keys.\n",
    "\n",
    "To use this dataset we must filter by the value of the key we want to use e.g. Large and plot the metric_coverage field. \n",
    "\n",
    "The dataset has 4 data points per metric, as we consider 4 past quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c3d12-b8ac-42da-a91f-dee98415ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Curve Fitting per Cut\n",
    "# for this to run I need to filter a) they aggregation level, b) the specific value at that aggregation level.\n",
    "# the result will be a table like key_agg, day_metric, \n",
    "\n",
    "#initialization\n",
    "combined_fitted_results = None\n",
    "\n",
    "#iterate through all the aggregated keys\n",
    "for agg_key_name in combined_agg_results.agg_key_name.unique():\n",
    "\n",
    "    #filter temporary data\n",
    "    temp_key_data = combined_agg_results[combined_agg_results['agg_key_name']==agg_key_name].copy()\n",
    "    \n",
    "    #iterate through the unique values of the specific aggregation\n",
    "    for agg_key_value in temp_key_data.agg_key_value.unique():\n",
    "        \n",
    "        # filter specific value per agg temporary data\n",
    "        temp_key_value_data = temp_key_data[temp_key_data['agg_key_value']==agg_key_value].copy()\n",
    "                \n",
    "        # set up the range of the quarter curves we want to report on\n",
    "        metrics_temp = pd.DataFrame({\"close_day_of_fiscal_quarter_normalised\":range(0,91)})\n",
    "\n",
    "        for metric_name in temp_key_value_data.metric_name.unique():\n",
    "            \n",
    "            # drop nas\n",
    "            temp_metric_data = temp_key_value_data[temp_key_value_data['metric_name']==metric_name].copy()\n",
    "            temp_metric_data.dropna(subset=['metric_coverage'], inplace=True)\n",
    "            temp_cuve = None\n",
    "            # avoid fitting curves for cuts that do not have enough data\n",
    "            if len(temp_metric_data) >= 90:     \n",
    "                temp_curve = fit_curve_to_agg (temp_metric_data,'close_day_of_fiscal_quarter_normalised','metric_coverage') \n",
    "                temp_curve.rename({'metric_coverage':metric_name+'_coverage'},inplace=1,axis=1)   \n",
    "                metrics_temp =  metrics_temp.merge(temp_curve, how='left', on='close_day_of_fiscal_quarter_normalised')                 \n",
    "            \n",
    "        # add the metric detail to the dataset\n",
    "        metrics_temp['agg_key_name'] = agg_key_name\n",
    "        metrics_temp['agg_key_value'] = agg_key_value\n",
    "        \n",
    "        #consolidate results\n",
    "        if combined_fitted_results is None:\n",
    "            combined_fitted_results = metrics_temp.copy()\n",
    "        else:\n",
    "            combined_fitted_results = combined_fitted_results.append(metrics_temp) \n",
    "        \n",
    "        # remove all nans so we can plot the charts\n",
    "        combined_fitted_results.dropna(inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8cefad-1a94-4fdf-a7cc-cbfe958036fb",
   "metadata": {},
   "source": [
    "# Linearity metric calculation - Data Extract & Fit\n",
    "\n",
    "For linearity we follow a similar process, preaggregate the metrics and then fit a curve to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54751a73-0bef-4320-8cd9-05dac918f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CQ data set is the only relevant for linearity calculation\n",
    "\n",
    "# columns to use for the group by (apart from the aggregation key)\n",
    "agg_columns = ['close_day_of_fiscal_quarter_normalised', 'close_fiscal_quarter_name']\n",
    "\n",
    "# fields to be summarized\n",
    "agg_fields = ['total_booked_net_arr', 'booked_net_arr']\n",
    "\n",
    "# initialize to none, add. intermediate dataframes\n",
    "# the goal is to calcualte the linearity for each aggregation key. First the data needs to be grouped\n",
    "# at that level and the relevant fields summed up. Then the coverage calculation function is used\n",
    "linearity_agg_results = None\n",
    "for agg_key in agg_key_list:\n",
    "    \n",
    "    # the key for aggregation needs to be complimented by the day key and the quarter key\n",
    "    total_agg_keys = [agg_key] + agg_columns\n",
    "\n",
    "    #calculate the aggregation per key\n",
    "    temp = cq_metrics_data.groupby(total_agg_keys,dropna=False)[agg_fields].agg('sum').reset_index().copy()\n",
    "        \n",
    "    # calculate coverages per metric, all are calculated using the \"Pending\" approach\n",
    "    # this needs to be done for every iteration \n",
    "    temp['bookings_linearity'] = temp.apply(calculate_bookings_linearity, axis=1,qtd_bookings='booked_net_arr',actual_booked='total_booked_net_arr')\n",
    "\n",
    "    # clean up / add information regarding the agg key\n",
    "    temp = temp.rename({agg_key:'agg_key_value'},axis=1)\n",
    "    temp['agg_key_name'] = agg_key\n",
    "\n",
    "    #consolidate results\n",
    "    if linearity_agg_results is None:\n",
    "        linearity_agg_results = temp.copy()\n",
    "    else:\n",
    "        linearity_agg_results = linearity_agg_results.append(temp) \n",
    "\n",
    "# set linearity 0 to NaN\n",
    "linearity_agg_results[linearity_agg_results['bookings_linearity']==0]=np.NaN\n",
    "\n",
    "# remove NaNs from dataset\n",
    "linearity_agg_results.dropna(subset=['bookings_linearity'],axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ca4c1-c26b-459f-b335-9d7184b66a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Curve Fitting per Cut\n",
    "# for this to run I need to filter a) they aggregation level, b) the specific value at that aggregation level.\n",
    "# the result will be a table like key_agg, day_metric, \n",
    "\n",
    "#initialization\n",
    "linearity_fitted_results = None\n",
    "\n",
    "#iterate through all the aggregated keys\n",
    "for agg_key_name in linearity_agg_results.agg_key_name.unique():\n",
    "\n",
    "    #filter temporary data\n",
    "    temp_key_data = linearity_agg_results[linearity_agg_results['agg_key_name']==agg_key_name].copy()\n",
    "    linearity_temp = None\n",
    "    \n",
    "    #iterate through the unique values of the specific aggregation\n",
    "    for agg_key_value in temp_key_data.agg_key_value.unique():\n",
    "        \n",
    "        # filter specific value per agg temporary data\n",
    "        temp_key_value_data = temp_key_data[temp_key_data['agg_key_value']==agg_key_value].copy()\n",
    "                \n",
    "        # set up the range of the quarter curves we want to report on\n",
    "        linearity_temp = pd.DataFrame({\"close_day_of_fiscal_quarter_normalised\":range(0,91)})\n",
    "                 \n",
    "        # drop nas\n",
    "        temp_linearity_data = temp_key_value_data.copy()\n",
    "        temp_cuve = None\n",
    "\n",
    "        # avoid fitting curves for cuts that do not have enough data\n",
    "        if len(temp_linearity_data) > 180:     \n",
    "            temp_curve = fit_curve_to_agg (temp_linearity_data,'close_day_of_fiscal_quarter_normalised','bookings_linearity') \n",
    "            linearity_temp =  linearity_temp.merge(temp_curve, how='left', on='close_day_of_fiscal_quarter_normalised')                 \n",
    "            \n",
    "        # add the metric detail to the dataset\n",
    "        linearity_temp['agg_key_name'] = agg_key_name\n",
    "        linearity_temp['agg_key_value'] = agg_key_value\n",
    "        \n",
    "        #consolidate results\n",
    "        if linearity_fitted_results is None:\n",
    "            linearity_fitted_results = linearity_temp.copy()\n",
    "        else:\n",
    "            linearity_fitted_results = linearity_fitted_results.append(linearity_temp) \n",
    "        \n",
    "# remove all nans so we can plot the charts\n",
    "linearity_fitted_results.dropna(subset=['bookings_linearity'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9ca18-808a-4546-b610-828d115622ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine with the coverage results\n",
    "combined_fitted_results = combined_fitted_results.merge(linearity_fitted_results, how='left',on=['close_day_of_fiscal_quarter_normalised','agg_key_name','agg_key_value'])\n",
    "combined_fitted_results.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db18e64e-51ed-4825-8e6c-b7a2da03f2db",
   "metadata": {},
   "source": [
    "# Save Quarterly METRICS to Gsheets & Drive\n",
    "\n",
    "The final goal is to get this process to run automatically once a quarter and save the results into a Snowflake database. Until that works, we save the curve results into a tab in the Source X-Ray file.\n",
    "\n",
    "Link to the file X-Ray Source file:\n",
    "\n",
    "- https://docs.google.com/spreadsheets/d/1Vwu8euxRgIF3QYWK8hAbp4Vy21AlFfpDwI4MaEEiIWk/edit#gid=930691697\n",
    "\n",
    "Link to the sheetload file:\n",
    "- https://docs.google.com/spreadsheets/d/1dLevdYA8QMjpIV9irNGD8KTfKIgc3xJdrRDA2vH7M50/edit#gid=856928614\n",
    "\n",
    "For the automatic upload on using DRIVE load from the Data Team, the column names MUST BE UPPERCASE.\n",
    "\n",
    "Upload issue: https://gitlab.com/gitlab-data/analytics/-/issues/12157\n",
    "\n",
    "The target folders are: \n",
    "\n",
    "- SS&A Coverage Fit\n",
    "https://drive.google.com/drive/folders/1jfIPIbYrNO7ApTphG_0qG-IkRVvpBn3W\n",
    "\n",
    "- SS&A Metric Data (Pre-Aggregated)\n",
    "https://drive.google.com/drive/folders/19H3G2gaIBmYppDiqRxB_ChCcGx4OIZeP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b22c31-c931-410e-8b2b-4c61208fb7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/59815620/gcloud-upload-httplib2-redirectmissinglocation-redirected-but-the-response-is-m\n",
    "!pip install httplib2==0.15.0\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "scope = [\"https://www.googleapis.com/auth/drive\"]\n",
    "gauth.credentials = ServiceAccountCredentials.from_json_keyfile_name(service_file_path, scope)\n",
    "drive = GoogleDrive(gauth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793fc2c-c503-4503-9700-571418acba33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Raw data used to calculate the fitted curves\n",
    "sheet_name = 'ssa_quarterly_aggregated_metrics_for_coverage'\n",
    "\n",
    "# set the agg_key_value to string so we can index jihu out\n",
    "\n",
    "combined_agg_results = combined_agg_results[index].dropna(subset=['agg_key_value'], how='all').copy()\n",
    "combined_agg_results['agg_key_value'] = combined_agg_results['agg_key_value'].astype(str)\n",
    "subset = ['agg_key_value','metric_value', 'total_booked_net_arr', 'booked_net_arr','metric_coverage']\n",
    "index = ~combined_agg_results['agg_key_value'].str.contains('jihu')\n",
    "for_gsheet = combined_agg_results[index].dropna(subset=subset, how='all').copy()\n",
    "\n",
    "# nans are not welcome in the drive upload process\n",
    "index = ~for_gsheet['agg_key_value'].str.contains('na')\n",
    "for_gsheet = for_gsheet[index].copy()\n",
    "\n",
    "#columns need to be upper case for the drive upload process to work\n",
    "for_gsheet.columns = for_gsheet.columns.str.upper()\n",
    "fields_order = ['AGG_KEY_VALUE', 'METRIC_NAME', 'CLOSE_DAY_OF_FISCAL_QUARTER_NORMALISED', 'CLOSE_FISCAL_QUARTER_NAME', 'METRIC_VALUE', 'TOTAL_BOOKED_NET_ARR', 'BOOKED_NET_ARR', 'METRIC_COVERAGE', 'AGG_KEY_NAME']\n",
    "for_gsheet = for_gsheet[fields_order].copy()\n",
    "\n",
    "# Store data in gsheets and CSV\n",
    "#if save_to_gsheets_and_csv:\n",
    "#    for_gsheet.to_csv('ssa_quarterly_aggregated_metrics_for_coverage_V2.csv',index=False) #ssa_quarterly_aggregated_metrics_for_coverage.csv\n",
    "\n",
    "# upload to drive folder for autoupload -> FILE IS TO BIG FOR THIS TO WORK\n",
    "#https://drive.google.com/file/d/13mjP-A831mTMlNWXNOA5EfRm_uoI5_Oo/view?usp=sharing\n",
    "\n",
    "#if upload_to_drive_csv:\n",
    "#    gfile = drive.CreateFile({'parents': [{'id': '19H3G2gaIBmYppDiqRxB_ChCcGx4OIZeP'}]})\n",
    "#    gfile.SetContentFile('ssa_quarterly_aggregated_metrics_for_coverage_V2.csv')\n",
    "#    gfile.Upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b563e-8334-4264-9102-7b4a852dd8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to the X-Ray source file, this is temporary while we do not have the data in the database\n",
    "sheet_name = 'hist_coverage_fitted_curves'\n",
    "\n",
    "subset = ['open_1plus_net_arr_coverage', 'open_3plus_net_arr_coverage','open_4plus_net_arr_coverage','rq_plus_1_open_1plus_net_arr_coverage','rq_plus_1_open_3plus_net_arr_coverage','rq_plus_1_open_4plus_net_arr_coverage', 'rq_plus_2_open_1plus_net_arr_coverage','rq_plus_2_open_3plus_net_arr_coverage','rq_plus_2_open_4plus_net_arr_coverage']\n",
    "index = ~combined_fitted_results['agg_key_value'].str.contains('jihu')\n",
    "for_gsheet = combined_fitted_results[index].dropna(subset=subset, how='all').copy()\n",
    "\n",
    "for_gsheet['key_agg_day'] = for_gsheet['agg_key_value'] + '_' + for_gsheet['close_day_of_fiscal_quarter_normalised'].astype(str) \n",
    "fields_order = ['key_agg_day','agg_key_name','agg_key_value','close_day_of_fiscal_quarter_normalised','bookings_linearity','open_1plus_net_arr_coverage', 'open_3plus_net_arr_coverage','open_4plus_net_arr_coverage','rq_plus_1_open_1plus_net_arr_coverage','rq_plus_1_open_3plus_net_arr_coverage','rq_plus_1_open_4plus_net_arr_coverage', 'rq_plus_2_open_1plus_net_arr_coverage','rq_plus_2_open_3plus_net_arr_coverage','rq_plus_2_open_4plus_net_arr_coverage']\n",
    "\n",
    "# columns in the CSV MUST BE UPPERCASE, if not, the drive process might fail\n",
    "for_gsheet = for_gsheet[fields_order].copy()\n",
    "for_gsheet.columns = for_gsheet.columns.str.upper()\n",
    "\n",
    "# Store data in gsheets and CSV\n",
    "if save_to_gsheets_and_csv:\n",
    "    for_gsheet.to_csv('ssa_coverage_fitted_curves.csv',index=False)\n",
    "#    write_to_gsheet(service_file_path, curves_sheetload_spreadsheet_id, sheet_name,for_gsheet)\n",
    "\n",
    "# upload to drive folder for autoupload\n",
    "# file link https://drive.google.com/file/d/1jM_ur0gVnhNjzQZobFfwWHGnS1jujnJg/view?usp=sharing\n",
    "\n",
    "if upload_to_drive_csv:\n",
    "    gfile = drive.CreateFile({'id':'1jM_ur0gVnhNjzQZobFfwWHGnS1jujnJg','parents': [{'id': '1jfIPIbYrNO7ApTphG_0qG-IkRVvpBn3W'}]})\n",
    "    gfile.SetContentFile('ssa_coverage_fitted_curves.csv')\n",
    "    gfile.Upload()\n",
    "    \n",
    "    \n",
    "#engine.push_to_snowflake('workspace_sales',for_gsheet,'')\n",
    "#pd.to_sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0d145-0699-4b37-852b-45dbb56ddd5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "-------------------------------\n",
    "# Calculation of FY Coverages & Curve Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01347b15-fb1d-4bde-8e16-b6f7babcaed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect yearly data\n",
    "sql_file = 'fy_metrics_by_day'\n",
    "\n",
    "load_db_data = False\n",
    "\n",
    "if load_db_data: \n",
    "    print (\"Executing {} Snowflake SQL\".format(sql_file))\n",
    "    fy_metrics_by_day = executeScriptFromFile(sql_file+'.sql', engine)\n",
    "    fy_metrics_by_day.to_csv(sql_file+'.csv',index=False)\n",
    "else:\n",
    "    print (\"Reading Local CSV {}.csv\".format(sql_file))\n",
    "    fy_metrics_by_day = pd.read_csv(sql_file+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ec8c9-cecc-4e95-aeae-5e1a1ad40321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "##### Data Injections\n",
    "\n",
    "# create an artificial global key to calculate a global curve for the whole company\n",
    "fy_metrics_by_day['key_overall'] = 'global'\n",
    "\n",
    "# adjust the sales_team legacy key to be sure that it won't generate duplicate with the FY23 key logic\n",
    "fy_metrics_by_day['sales_team_rd_asm_level'] = 'st_rd_' + fy_metrics_by_day['sales_team_rd_asm_level']\n",
    "\n",
    "# add a commercial and enterprise consolidated key\n",
    "#add key enterprise commercial for x-ray reporting\n",
    "fy_metrics_by_day['key_ent_comm'] = 'other'\n",
    "fy_metrics_by_day.loc[fy_metrics_by_day['key_segment']=='large','key_ent_comm'] = 'enterprise'\n",
    "fy_metrics_by_day.loc[fy_metrics_by_day['key_segment']=='pubsec','key_ent_comm'] = 'enterprise'\n",
    "fy_metrics_by_day.loc[fy_metrics_by_day['key_segment']=='mid-market','key_ent_comm'] = 'commercial'\n",
    "fy_metrics_by_day.loc[fy_metrics_by_day['key_segment']=='smb','key_ent_comm'] = 'commercial'\n",
    "\n",
    "# calculate the Fiscal Years to be considered on the transformation\n",
    "max_fy_year = fy_metrics_by_day.close_fiscal_year.sort_values().unique().max() \n",
    "considered_fiscal_years = [max_fy_year - 1,max_fy_year -2]\n",
    "considered_fiscal_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c99acd3-7569-4d6b-b6ad-f5c03e9b58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture relevant data\n",
    "cfy_metrics_data = fy_metrics_by_day[fy_metrics_by_day['close_fiscal_year'].isin(considered_fiscal_years)].copy()\n",
    "\n",
    "#add fiscal year total\n",
    "index = (cfy_metrics_data['close_day_of_fiscal_year_normalised']==365)\n",
    "fy_total = cfy_metrics_data[index][['report_user_segment_geo_region_area_sqs_ot','close_fiscal_year','cfy_booked_net_arr']]\n",
    "fy_total.rename(columns={'cfy_booked_net_arr':'total_fy_booked_net_arr'},inplace=True)\n",
    "# merge total per year\n",
    "cfy_metrics_data = cfy_metrics_data.merge (fy_total, how='left', on = ['report_user_segment_geo_region_area_sqs_ot','close_fiscal_year'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da0d7a-c4dc-467a-a78a-3b02bce187f5",
   "metadata": {},
   "source": [
    "### Considered metrics\n",
    "\n",
    "For FY & 4Q we could consider Next period perspective from current point in time. Specially interesting for FY. But as of today, we only have 1 full FY to use and that is not nearly enough.\n",
    "\n",
    "Already for current FY reporting we only have 2 valid FYs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e1074b-7f8a-42fe-8790-ed4595004a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# move from wide into tall dataset\n",
    "print(agg_key_list) # considered keys\n",
    "\n",
    "### current fiscal year\n",
    "variables = ['close_fiscal_year','close_day_of_fiscal_year_normalised','cfy_booked_net_arr','total_fy_booked_net_arr'] + agg_key_list\n",
    "considered_metrics = ['cfy_open_1plus_net_arr','cfy_open_3plus_net_arr'] \n",
    "fy_melt = cfy_metrics_data.melt(id_vars = variables, value_vars = considered_metrics)\n",
    "\n",
    "fy_melt = fy_melt.rename({'variable':'metric_name'}, axis=1)\n",
    "fy_melt = fy_melt.rename({'value':'metric_value'}, axis=1)\n",
    "fy_melt.dropna(subset=['total_fy_booked_net_arr', 'metric_value'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb067813-7e2c-4151-8e47-3e7029491f04",
   "metadata": {},
   "source": [
    "### Calculate coverages for all business cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa6f06b-08bd-47f8-b5d6-3d41912f188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation columns\n",
    "agg_columns = ['metric_name','close_day_of_fiscal_year_normalised', 'close_fiscal_year']\n",
    "# fields to be summarized\n",
    "agg_fields = ['metric_value','total_fy_booked_net_arr', 'cfy_booked_net_arr']\n",
    "\n",
    "# initialize to none, add. intermediate dataframes\n",
    "# the goal is to calcualte the coverage for each aggregation key. First the data needs to be grouped\n",
    "# at that level and the relevant fields summed up. Then the coverage calculation function is used\n",
    "combined_agg_results = None\n",
    "for agg_key in agg_key_list:\n",
    "    \n",
    "    # the key for aggregation needs to be complimented by the day key and the quarter key\n",
    "    total_agg_keys = [agg_key] + agg_columns\n",
    "\n",
    "    #calculate the aggregation per key\n",
    "    temp = fy_melt.groupby(total_agg_keys,dropna=False)[agg_fields].agg('sum').reset_index().copy()\n",
    "        \n",
    "    # calculate coverages per metric, all are calculated using the \"Pending\" approach\n",
    "    # this needs to be done for every iteration \n",
    "    temp['metric_coverage'] = temp.apply(calculate_to_pending_coverage, axis=1,metric='metric_value',qtd='cfy_booked_net_arr',actual='total_fy_booked_net_arr')\n",
    "\n",
    "    # clean up / add information regarding the agg key\n",
    "    temp = temp.rename({agg_key:'agg_key_value'},axis=1)\n",
    "    temp['agg_key_name'] = agg_key\n",
    "\n",
    "    #consolidate results\n",
    "    if combined_agg_results is None:\n",
    "        combined_agg_results = temp.copy()\n",
    "    else:\n",
    "        combined_agg_results = combined_agg_results.append(temp) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4833a52b-9555-4ddb-b83a-d52860e9da26",
   "metadata": {},
   "source": [
    "### Calculate Curve Fit for FY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5858391f-a042-4a05-b420-1f2029f97be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Curve Fitting per Cut\n",
    "# for this to run I need to filter a) they aggregation level, b) the specific value at that aggregation level.\n",
    "# the result will be a table like key_agg, day_metric, \n",
    "\n",
    "#initialization\n",
    "combined_fitted_results = None\n",
    "\n",
    "#iterate through all the aggregated keys\n",
    "for agg_key_name in combined_agg_results.agg_key_name.unique():\n",
    "\n",
    "    #filter temporary data\n",
    "    temp_key_data = combined_agg_results[combined_agg_results['agg_key_name']==agg_key_name].copy()\n",
    "    \n",
    "    #iterate through the unique values of the specific aggregation\n",
    "    for agg_key_value in temp_key_data.agg_key_value.unique():\n",
    "        \n",
    "        # filter specific value per agg temporary data\n",
    "        temp_key_value_data = temp_key_data[temp_key_data['agg_key_value']==agg_key_value].copy()\n",
    "                \n",
    "        # set up the range of the quarter curves we want to report on\n",
    "        metrics_temp = pd.DataFrame({\"close_day_of_fiscal_year_normalised\":range(0,355)})\n",
    "\n",
    "        for metric_name in temp_key_value_data.metric_name.unique():\n",
    "            \n",
    "            # drop nas\n",
    "            temp_metric_data = temp_key_value_data[temp_key_value_data['metric_name']==metric_name].copy()\n",
    "            temp_metric_data.dropna(subset=['metric_coverage'], inplace=True)\n",
    "            temp_cuve = None\n",
    "            # avoid fitting curves for cuts that do not have enough data\n",
    "            if len(temp_metric_data) >= 90:     \n",
    "                temp_curve = fit_curve_to_agg (temp_metric_data,'close_day_of_fiscal_year_normalised','metric_coverage') \n",
    "                temp_curve.rename({'metric_coverage':metric_name+'_coverage'},inplace=1,axis=1)   \n",
    "                metrics_temp =  metrics_temp.merge(temp_curve, how='left', on='close_day_of_fiscal_year_normalised')                 \n",
    "            \n",
    "        # add the metric detail to the dataset\n",
    "        metrics_temp['agg_key_name'] = agg_key_name\n",
    "        metrics_temp['agg_key_value'] = agg_key_value\n",
    "        \n",
    "        #consolidate results\n",
    "        if combined_fitted_results is None:\n",
    "            combined_fitted_results = metrics_temp.copy()\n",
    "        else:\n",
    "            combined_fitted_results = combined_fitted_results.append(metrics_temp) \n",
    "        \n",
    "        # remove all nans so we can plot the charts\n",
    "        combined_fitted_results.dropna(inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc07785-3098-4ee3-8f9b-e4b55a5a87b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fy_combined_fitted_results = combined_fitted_results.copy()\n",
    "fy_combined_agg_results = combined_agg_results.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5cbd5b-5c36-4bc9-aa8e-e534a6b7d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key_value_filter = 'large_apac_apac_korea_2. growth'\n",
    "key_value_filter = 'large_emea'\n",
    "metric_value_filter = 'cfy_open_1plus_net_arr'\n",
    "\n",
    "combined_agg_results.head()\n",
    "index = (combined_agg_results['agg_key_value']==key_value_filter) & (combined_agg_results['metric_name']==metric_value_filter) #& (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_agg_results[index]\n",
    "sns.lineplot(data=temp,hue='close_fiscal_year', y='metric_coverage',x='close_day_of_fiscal_year_normalised')\n",
    "index = (combined_fitted_results['agg_key_value']==key_value_filter) #& (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_fitted_results[index]\n",
    "sns.lineplot(data=temp, y=metric_value_filter + '_coverage',x='close_day_of_fiscal_year_normalised',color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db73ccf-e034-47df-9811-0c06eeb6b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fitted_results.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4555f6a-623e-46b9-b084-e200c0b14876",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "# 4Q Coverage Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c3acc-78db-446f-95d7-07e0b7d2dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect yearly data\n",
    "sql_file = 'metrics_4q_by_day'\n",
    "\n",
    "load_db_data = True\n",
    "\n",
    "if load_db_data: \n",
    "    print (\"Executing {} Snowflake SQL\".format(sql_file))\n",
    "    metrics_4q_by_day = executeScriptFromFile(sql_file+'.sql', engine)\n",
    "    metrics_4q_by_day.to_csv(sql_file+'.csv',index=False)\n",
    "else:\n",
    "    print (\"Reading Local CSV {}.csv\".format(sql_file))\n",
    "    metrics_4q_by_day = pd.read_csv(sql_file+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e6e49-6d33-4c2b-aef3-bc0bae9c3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "##### Data Injections\n",
    "\n",
    "# create an artificial global key to calculate a global curve for the whole company\n",
    "metrics_4q_by_day['key_overall'] = 'global'\n",
    "\n",
    "# adjust the sales_team legacy key to be sure that it won't generate duplicate with the FY23 key logic\n",
    "metrics_4q_by_day['sales_team_rd_asm_level'] = 'st_rd_' + metrics_4q_by_day['sales_team_rd_asm_level']\n",
    "\n",
    "# add a commercial and enterprise consolidated key\n",
    "#add key enterprise commercial for x-ray reporting\n",
    "metrics_4q_by_day['key_ent_comm'] = 'other'\n",
    "metrics_4q_by_day.loc[metrics_4q_by_day['key_segment']=='large','key_ent_comm'] = 'enterprise'\n",
    "metrics_4q_by_day.loc[metrics_4q_by_day['key_segment']=='pubsec','key_ent_comm'] = 'enterprise'\n",
    "metrics_4q_by_day.loc[metrics_4q_by_day['key_segment']=='mid-market','key_ent_comm'] = 'commercial'\n",
    "metrics_4q_by_day.loc[metrics_4q_by_day['key_segment']=='smb','key_ent_comm'] = 'commercial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678cbfae-19ae-45a4-8de0-c1b490f20698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter considered quarters\n",
    "index_cond = (metrics_4q_by_day['end_fiscal_quarter_date'] <= date.today() + relativedelta(months=-3)) & (metrics_4q_by_day['end_fiscal_quarter_date'] >= date.today() + relativedelta(months=-15))\n",
    "cq_considered_quarters = metrics_4q_by_day[index_cond].end_fiscal_quarter_name.sort_values().unique()\n",
    "cq_considered_quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284dc781-3069-4861-a680-b1a6340b6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture relevant data\n",
    "metrics_4q_data = metrics_4q_by_day[metrics_4q_by_day['end_fiscal_quarter_name'].isin(cq_considered_quarters)].copy()\n",
    "\n",
    "# move from wide into tall dataset\n",
    "print(agg_key_list) # considered keys\n",
    "\n",
    "### current fiscal year\n",
    "variables = ['start_fiscal_quarter_name','day_of_combined_4q_normalised','n4q_booked_net_arr','total_n4q_booked_net_arr'] + agg_key_list\n",
    "considered_metrics = ['n4q_open_1plus_net_arr','n4q_open_3plus_net_arr'] \n",
    "melt_4q = metrics_4q_data.melt(id_vars = variables, value_vars = considered_metrics)\n",
    "\n",
    "melt_4q = melt_4q.rename({'variable':'metric_name'}, axis=1)\n",
    "melt_4q = melt_4q.rename({'value':'metric_value'}, axis=1)\n",
    "melt_4q.dropna(subset=['total_n4q_booked_net_arr', 'metric_value'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cdca67-b455-4e58-b96d-2ead24af00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation columns\n",
    "agg_columns = ['metric_name','day_of_combined_4q_normalised', 'start_fiscal_quarter_name']\n",
    "# fields to be summarized\n",
    "agg_fields = ['metric_value','total_n4q_booked_net_arr', 'n4q_booked_net_arr']\n",
    "\n",
    "# initialize to none, add. intermediate dataframes\n",
    "# the goal is to calcualte the coverage for each aggregation key. First the data needs to be grouped\n",
    "# at that level and the relevant fields summed up. Then the coverage calculation function is used\n",
    "combined_agg_results = None\n",
    "for agg_key in agg_key_list:\n",
    "    \n",
    "    # the key for aggregation needs to be complimented by the day key and the quarter key\n",
    "    total_agg_keys = [agg_key] + agg_columns\n",
    "\n",
    "    #calculate the aggregation per key\n",
    "    temp = melt_4q.groupby(total_agg_keys,dropna=False)[agg_fields].agg('sum').reset_index().copy()\n",
    "        \n",
    "    # calculate coverages per metric, all are calculated using the \"Pending\" approach\n",
    "    # this needs to be done for every iteration \n",
    "    temp['metric_coverage'] = temp.apply(calculate_to_pending_coverage, axis=1,metric='metric_value',qtd='n4q_booked_net_arr',actual='total_n4q_booked_net_arr')\n",
    "\n",
    "    # clean up / add information regarding the agg key\n",
    "    temp = temp.rename({agg_key:'agg_key_value'},axis=1)\n",
    "    temp['agg_key_name'] = agg_key\n",
    "\n",
    "    #consolidate results\n",
    "    if combined_agg_results is None:\n",
    "        combined_agg_results = temp.copy()\n",
    "    else:\n",
    "        combined_agg_results = combined_agg_results.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975f46e-16f0-43b4-8e8c-7533838d33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Curve Fitting per Cut\n",
    "# for this to run I need to filter a) they aggregation level, b) the specific value at that aggregation level.\n",
    "# the result will be a table like key_agg, day_metric, \n",
    "\n",
    "#initialization\n",
    "combined_fitted_results = None\n",
    "\n",
    "#iterate through all the aggregated keys\n",
    "for agg_key_name in combined_agg_results.agg_key_name.unique():\n",
    "\n",
    "    #filter temporary data\n",
    "    temp_key_data = combined_agg_results[combined_agg_results['agg_key_name']==agg_key_name].copy()\n",
    "    \n",
    "    #iterate through the unique values of the specific aggregation\n",
    "    for agg_key_value in temp_key_data.agg_key_value.unique():\n",
    "        \n",
    "        # filter specific value per agg temporary data\n",
    "        temp_key_value_data = temp_key_data[temp_key_data['agg_key_value']==agg_key_value].copy()\n",
    "                \n",
    "        # set up the range of the quarter curves we want to report on\n",
    "        metrics_temp = pd.DataFrame({\"day_of_combined_4q_normalised\":range(0,355)})\n",
    "\n",
    "        for metric_name in temp_key_value_data.metric_name.unique():\n",
    "            \n",
    "            # drop nas\n",
    "            temp_metric_data = temp_key_value_data[temp_key_value_data['metric_name']==metric_name].copy()\n",
    "            temp_metric_data.dropna(subset=['metric_coverage'], inplace=True)\n",
    "            temp_cuve = None\n",
    "            # avoid fitting curves for cuts that do not have enough data\n",
    "            if len(temp_metric_data) >= 90:     \n",
    "                temp_curve = fit_curve_to_agg (temp_metric_data,'day_of_combined_4q_normalised','metric_coverage') \n",
    "                temp_curve.rename({'metric_coverage':metric_name+'_coverage'},inplace=1,axis=1)   \n",
    "                metrics_temp =  metrics_temp.merge(temp_curve, how='left', on='day_of_combined_4q_normalised')                 \n",
    "            \n",
    "        # add the metric detail to the dataset\n",
    "        metrics_temp['agg_key_name'] = agg_key_name\n",
    "        metrics_temp['agg_key_value'] = agg_key_value\n",
    "        \n",
    "        #consolidate results\n",
    "        if combined_fitted_results is None:\n",
    "            combined_fitted_results = metrics_temp.copy()\n",
    "        else:\n",
    "            combined_fitted_results = combined_fitted_results.append(metrics_temp) \n",
    "        \n",
    "        # remove all nans so we can plot the charts\n",
    "        combined_fitted_results.dropna(inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce122f7-9008-4d09-b0f8-39c55e13b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "n4q_combined_fitted_results = combined_fitted_results.copy()\n",
    "n4q_combined_agg_results = combined_agg_results.copy()\n",
    "\n",
    "n4q_combined_fitted_results.rename(columns={'day_of_combined_4q_normalised':'close_day_of_fiscal_year_normalised'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb69b6a-d087-4ce2-963b-023c1ea080c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key_value_filter = 'large_apac_apac_korea_2. growth'\n",
    "key_value_filter = 'large_emea'\n",
    "metric_value_filter = 'n4q_open_1plus_net_arr'\n",
    "\n",
    "combined_agg_results.head()\n",
    "index = (combined_agg_results['agg_key_value']==key_value_filter) & (combined_agg_results['metric_name']==metric_value_filter) #& (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_agg_results[index]\n",
    "sns.lineplot(data=temp,hue='start_fiscal_quarter_name', y='metric_coverage',x='day_of_combined_4q_normalised')\n",
    "index = (combined_fitted_results['agg_key_value']==key_value_filter) #& (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_fitted_results[index]\n",
    "sns.lineplot(data=temp, y=metric_value_filter + '_coverage',x='day_of_combined_4q_normalised',color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf29d03b-d7fa-4fc6-9fdd-5eff9ed6357f",
   "metadata": {},
   "source": [
    "# FY & 4Q Copy to gSheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99aecd7-e369-490c-991a-9096e6b390d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for_gsheet = fy_combined_fitted_results.copy()\n",
    "for_gsheet['agg_key_value'] = for_gsheet['agg_key_value'].astype(str)\n",
    "subset = ['cfy_open_1plus_net_arr_coverage', 'cfy_open_3plus_net_arr_coverage', 'agg_key_name', 'agg_key_value']\n",
    "\n",
    "# remove jihu\n",
    "index = ~for_gsheet['agg_key_value'].str.contains('jihu')\n",
    "for_gsheet = for_gsheet[index].dropna(subset=subset, how='all').copy()\n",
    "\n",
    "# remove other\n",
    "index = ~for_gsheet['agg_key_value'].str.contains('other')\n",
    "for_gsheet = for_gsheet[index].dropna(subset=subset, how='all').copy()\n",
    "\n",
    "# nans are not welcome in the drive upload process\n",
    "index = ~for_gsheet['agg_key_value'].str.contains('na')\n",
    "for_gsheet = for_gsheet[index].copy()\n",
    "\n",
    "#add key_agg_day\n",
    "for_gsheet['key_agg_day'] = for_gsheet['agg_key_value'] + '_' + for_gsheet['close_day_of_fiscal_year_normalised'].astype(str) \n",
    "\n",
    "# merge the 4Q columns into the FY extract\n",
    "subset = ['n4q_open_1plus_net_arr','n4q_open_3plus_net_arr', 'agg_key_name', 'agg_key_value']\n",
    "for_gsheet = for_gsheet.merge(n4q_combined_fitted_results, how='left',on=['agg_key_value','agg_key_name', 'close_day_of_fiscal_year_normalised'])\n",
    "\n",
    "print(for_gsheet.columns)\n",
    "\n",
    "# define the field order\n",
    "fields_order = ['key_agg_day', 'agg_key_value', 'close_day_of_fiscal_year_normalised', 'cfy_open_1plus_net_arr_coverage', 'cfy_open_3plus_net_arr_coverage','n4q_open_1plus_net_arr_coverage','n4q_open_3plus_net_arr_coverage']\n",
    "for_gsheet = for_gsheet[fields_order].copy()\n",
    "\n",
    "#columns need to be upper case for the drive upload process to work\n",
    "for_gsheet.columns = for_gsheet.columns.str.upper()\n",
    "for_gsheet.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75473b7e-13ae-492d-bb9a-41981ab60fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data in gsheets and CSV\n",
    "if save_to_gsheets_and_csv:\n",
    "    for_gsheet.to_csv('fy_coverage_fitted_curves.csv',index=False) #fy_coverage_fitted_curves.csv\n",
    "    curves_sheetload_spreadsheet_id = \"1Vwu8euxRgIF3QYWK8hAbp4Vy21AlFfpDwI4MaEEiIWk\"\n",
    "    sheet_name = \"next_4q_hist_cov_example\"\n",
    "    write_to_gsheet(service_file_path, curves_sheetload_spreadsheet_id, sheet_name,for_gsheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc35fc-fa6e-477e-a26c-a6826c3073d6",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d5aea-2eda-4475-b3aa-d78474e89f1e",
   "metadata": {},
   "source": [
    "# Test Code\n",
    "\n",
    "The following snippets were used during development to test the approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58504ec-047c-432a-9b73-f78e89e407b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that results make sense\n",
    "cq_plus2_metrics_data.groupby(['close_fiscal_quarter_name','close_day_of_fiscal_quarter_normalised'])['rq_plus_2_open_1plus_net_arr'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21acac-bc99-4f93-8e77-3a700c2823ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Test that the totals work. E.g. FY22Q4 Large = 29043057\n",
    "# combined_df[(combined_df['close_day_of_fiscal_quarter_normalised']==40) & (combined_df['metric']=='open_1plus_net_arr')].groupby(by=['key_segment','close_fiscal_quarter_name']).total_booked_net_arr.sum()\n",
    "\n",
    "####### Test that the coverage curve makes sense for one metric\n",
    "index = (combined_df['key_segment']=='large') & (combined_df['metric_name']=='open_1plus_net_arr') & (combined_df['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_df[index].groupby(['close_day_of_fiscal_quarter_normalised','key_segment']).agg({'total_booked_net_arr':'sum','metric_value':'sum','booked_net_arr':'sum'}).reset_index()\n",
    "\n",
    "# calculate coverages per metric, all are calculated using the \"Pending\" approach\n",
    "# this needs to be done for every iteration \n",
    "temp['metric_coverage'] = temp.apply(calculate_to_pending_coverage, axis=1,metric='metric_value',qtd='booked_net_arr',actual='total_booked_net_arr')\n",
    "\n",
    "sns.lineplot(data=temp,hue='key_segment', y='metric_coverage',x='close_day_of_fiscal_quarter_normalised')\n",
    "\n",
    "#### Test #1 that combined pre-grouped dataset has reasonable values\n",
    "combined_agg_results.head()\n",
    "index = (combined_agg_results['agg_key_value']=='large') & (combined_agg_results['metric_name']=='open_1plus_net_arr') & (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_agg_results[index]\n",
    "temp.head()\n",
    "sns.lineplot(data=temp,hue='agg_key_value', y='metric_coverage',x='close_day_of_fiscal_quarter_normalised')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c7a4b-f314-409d-8f1c-dd736fcf4b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Test #2 that combined pre-grouped dataset has reasonable values\n",
    "\n",
    "#key_value_filter = 'large_apac_apac_korea_2. growth'\n",
    "key_value_filter = 'large'\n",
    "metric_value_filter = 'rq_plus_2_open_1plus_net_arr'\n",
    "\n",
    "combined_agg_results.head()\n",
    "index = (combined_agg_results['agg_key_value']==key_value_filter) & (combined_agg_results['metric_name']==metric_value_filter) #& (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_agg_results[index]\n",
    "sns.lineplot(data=temp,hue='close_fiscal_quarter_name', y='metric_coverage',x='close_day_of_fiscal_quarter_normalised')\n",
    "index = (combined_fitted_results['agg_key_value']==key_value_filter) #& (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_fitted_results[index]\n",
    "sns.lineplot(data=temp, y=metric_value_filter + '_coverage',x='close_day_of_fiscal_quarter_normalised',color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e6fc73-d255-4a16-aeea-f40245a85be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test #2a that combined pre-grouped dataset has reasonable values\n",
    "\n",
    "key_value_filter = 'large_apac_apac_korea_2. growth'\n",
    "metric_value_filter = 'bookings_linearity'\n",
    "\n",
    "combined_agg_results.head()\n",
    "index = (combined_agg_results['agg_key_value']==key_value_filter) & (combined_agg_results['metric_name']==metric_value_filter) #& (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_agg_results[index]\n",
    "sns.lineplot(data=temp,hue='close_fiscal_quarter_name', y='metric_coverage',x='close_day_of_fiscal_quarter_normalised')\n",
    "combined_fitted_results.head()\n",
    "index = (combined_fitted_results['agg_key_value']==key_value_filter) #& (combined_agg_results['close_fiscal_quarter_name']=='FY22-Q1')\n",
    "temp = combined_fitted_results[index]\n",
    "sns.lineplot(data=temp, y=metric_value_filter,x='close_day_of_fiscal_quarter_normalised',color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa111d5-dd34-4d98-943c-3733048a4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test #3 that combined pre-grouped dataset has reasonable values\n",
    "combined_fitted_results.head()\n",
    "index = (combined_fitted_results['agg_key_value']=='large_amer_west')\n",
    "temp = combined_fitted_results[index]\n",
    "temp.head()\n",
    "sns.lineplot(data=temp,hue='agg_key_value', y='open_1plus_net_arr_coverage',x='close_day_of_fiscal_quarter_normalised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f14e7a-3489-47b0-9fa4-eab2ea400c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fitted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c940b9d-12da-48ff-9cc2-e45ab737d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_agg_results[combined_agg_results['agg_key_value']=='pubsec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee30652-8f10-41c6-84a6-110b3d3486a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_fitted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf255a0-84ac-4acf-9a37-52b70cc9dc79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28a29e-45d1-4ce2-9780-a16affebaa18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea4f0e1d1faa5bb96f695d7e31ce7a9817f73b9836258768e692d46b900aef52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
